{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOTQwCI8TvWyV/28o4DmqoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA-N-w_KNTqx"
      },
      "source": [
        "#[CS4740 Labs] Week 2: Basics of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDVelHbR_KqJ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab we will be carrying out some simple Machine Learning experiments in order to gain some familiarity with the Python ML ecosystem and Pytorch in particular. We will be using the Iris dataset that consists of 150 datapoints, each with 4 measurements and belonging to one out of 3 possible classes (varieties of Iris). As we will see in class, Logistic Regression is a probabilistic model. It defines the probability of a datapoint $\\mathbf{x}$ belonging to class $t$ as $$p(t|\\mathbf{x},W)=y_{t}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDKBLc-b3WsZ"
      },
      "source": [
        "where the $y_t$ are elements of the soft-max vector $$\\mathbf{y}=\\frac{1}{e^{\\mathbf{w}_{1}^{T}\\mathbf{x}}+\\ldots+e^{\\mathbf{w}_{C}^{T}\\mathbf{x}}}\\left[\\begin{array}{c}\n",
        "e^{\\mathbf{w}_{1}^{T}\\mathbf{x}}\\\\\n",
        "\\vdots\\\\\n",
        "e^{\\mathbf{w}_{C}^{T}\\mathbf{x}}\n",
        "\\end{array}\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rygf7panFeTK"
      },
      "source": [
        "In this formula the $\\mathbf{w}_1,\\ldots,\\mathbf{w}_C$ are the weights of the model with $C$ denoting the number of classes; in our case, $3$. Note that all $y_i$s are numbers between $0$ and $1$ that also add up to $1$, i.e. a probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpPLExcWILin"
      },
      "source": [
        "To estimate the values of the weights we minimize the negative log-likellihood of the data, given by\n",
        "\n",
        "\\begin{align*}\n",
        "L(\\mathbf{x},t;W) & =-\\log p(t|\\mathbf{x},W)\\\\\n",
        " & =-\\log y_{t}\\\\\n",
        " & =-\\log\\frac{e^{\\mathbf{w}_{t}^{T}\\mathbf{x}}}{e^{\\mathbf{w}_{1}^{T}\\mathbf{x}}+\\ldots+e^{\\mathbf{w}_{C}^{T}\\mathbf{x}}}\\\\\n",
        " & =\\log(e^{\\mathbf{w}_{1}^{T}\\mathbf{x}}+\\ldots+e^{\\mathbf{w}_{C}^{T}\\mathbf{x}})-\\mathbf{w}_{t}^{T}\\mathbf{x}\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8QEflgjv9vG"
      },
      "source": [
        "## Section 1. Hard-coding the gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QimU0iBK8MXD"
      },
      "source": [
        "The negative log-likelihood in Logistic Regression is a complicated function whose minimum cannot be computed analytically. We therefore have to use approximate optimization and one of the tried and tested ways of doing this is the Gradient Descent algorithm. As we will see in class, this can be loosely summarized as:\n",
        "\n",
        "```\n",
        "while not converged:\n",
        "    compute gradient of error at w\n",
        "    move w along the direction of the negative gradient by a small amount\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIVws_rM_w6j"
      },
      "source": [
        "As we saw above, the negative log likelihood consists of two terms which we can differentiate independently. Differentiating the first term gives \n",
        "\n",
        "$$\\begin{align*}\n",
        "\\nabla_{\\mathbf{w_{k}}}\\log(e^{\\mathbf{w}_{1}^{T}\\mathbf{x}}+\\ldots+e^{\\mathbf{w}_{C}^{T}\\mathbf{x}}) & =\\frac{e^{\\mathbf{w}_{k}^{T}\\mathbf{x}}}{e^{\\mathbf{w}_{1}^{T}\\mathbf{x}}+\\ldots+e^{\\mathbf{w}_{C}^{T}\\mathbf{x}}}\\mathbf{x}\\\\\n",
        " & =y_{k}\\mathbf{x}\n",
        "\\end{align*}.$$\n",
        "\n",
        "Differentiating the second term gives\n",
        "$$\\nabla_{\\mathbf{w_{k}}}\\mathbf{w}_{t}^{T}\\mathbf{x}=\\begin{cases}\n",
        "\\mathbf{x} & \\textrm{if}\\quad k=t\\\\\n",
        "0 & \\textrm{otherwise}\n",
        "\\end{cases}.$$\n",
        "\n",
        "Adding the two together we get\n",
        "\n",
        "$$\\nabla_{\\mathbf{w_{k}}}L(\\mathbf{x},t;W)=\\begin{cases}\n",
        "(y_{k}-1)\\mathbf{x} & \\textrm{if}\\quad k=t\\\\\n",
        "y_{k}\\mathbf{x} & \\textrm{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "for $k=1,\\ldots,C$. Bear in mind this analysis was for a single data point. The negative log likelihood is added across all data-points so we need to loop through the data and add all the gradients.\n",
        "\n",
        "First thing's first. Let's load up the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLYEIuDoi6wY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "T = iris.target\n",
        "\n",
        "C= len(set(list(T)))\n",
        "F = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjVg4AwGA5uj"
      },
      "source": [
        "So we loaded the $\\mathbf{x}$s in variable `X` and the labels $t$ in variable `T`. We also calculate the number of all possible classes `C` (in our case the classes are `{0,1,2}` so `C=3`) and the number of dimensions (features) in each datapoint `F` (in our case `F=4`). Now it's always a good idea to compute one term of the neg. log likelihood before we scale up to the whole thing. So let's define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiGHCQqHD9L7"
      },
      "source": [
        "x=X[0]\n",
        "t=T[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Rd5Gd4EDPQ"
      },
      "source": [
        "It will be very convenient to store the weights of the model in a single array `W` which has `C` rows and `F` columns. Let's initialize it randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frWGJ042EsOW"
      },
      "source": [
        "W = np.random.randn(C,F)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-G_RzFqE0WY"
      },
      "source": [
        "The matrix multiplication $W \\mathbf{x}$ gives the linear activations, i.e. a vector with elements $\\mathbf{w}_1^T \\mathbf{x},\\ldots,\\mathbf{w}_C^T \\mathbf{x}$ before they get passed to the soft-max function. The soft-max vector itself can be computed using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSdEnvBTGGtI"
      },
      "source": [
        "p = np.exp(W @ x)\n",
        "y = p/sum(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xh3tVT3GXRk"
      },
      "source": [
        "Let's verify that it is a probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U5rP0wRGWc5"
      },
      "source": [
        "print(y)\n",
        "print(sum(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ4lb2GBHT2f"
      },
      "source": [
        "Now the neg. log likelihood _just_ for this datapoint can be defined as "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVHX3KLqHbUN"
      },
      "source": [
        "L = - np.log(y[t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjLxPiGbGpbD"
      },
      "source": [
        "Let's now define a variable to hold the gradient of $L$ with respect to the weights, i.e. $\\nabla_W L$ which we will initialize to zero. Note that it has exactly the same dimensions as $W$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8NCkVIYHqVk"
      },
      "source": [
        "dW = np.zeros_like(W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOM7dHraH4WZ"
      },
      "source": [
        "According to the formula we derived above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qiOTU_rHwdg"
      },
      "source": [
        "for c in range(C):\n",
        "    dW[c] = (y[c] - 1) * x if c==t else y[c] * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhEbLCzGI3OT"
      },
      "source": [
        "Now all we need to do is loop through the whole dataset, adding the gradients as we go along. We will finally update the weights by adding a small negative multiple of the gradient, which is guaranteed to decrease the total neg. log likelihood by a small amount. It's also a good idea for sanity checking and general good practice, to produce some plot of the (hopefully) decreasing $L$. Adding everything together we get the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WU_7tpm0mWb"
      },
      "source": [
        "Ls=[] # this array will hold the neg. log likelihood as it decreases throughout \n",
        "# the algorithm\n",
        "W = np.random.randn(C,F) # Our model weights, randomly initialized\n",
        "alpha = 5e-4 # The learning rate\n",
        "for e in range(1000): # We will perform 1000 steps of gradient descent\n",
        "  dW = np.zeros_like(W) #initialize gradient to zero\n",
        "  L=0 #initialize the neg.log likelihood sum\n",
        "  for x,t in zip(X,T): #loop through dataset\n",
        "    p = np.exp(W @ x) #soft-max\n",
        "    y = p/sum(p) \n",
        "    L += -np.log(y[t]) #add neg. log. likelihood for datapoint\n",
        "    for c in range(C):# compute gradient for datapoint\n",
        "        dW[c] += (y[c] - 1) * x if c==t else y[c] * x    \n",
        "  W -= alpha*dW # move weights in direction oposite to gradient\n",
        "  Ls.append(L) # keep record of neg. log likelihood\n",
        "plt.plot(Ls,'r-') # plot trajectory of L during gradient descent **V. IMPORTANT**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgoFo3b3Ln28"
      },
      "source": [
        "Seems that we managed to decrease something! Let's see if this corresponds to some accurate predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9aBQfKBYyFH"
      },
      "source": [
        "correct = 0\n",
        "for x,t in zip(X,T):\n",
        "  a_pred = W @ x\n",
        "  correct += (a_pred.argmax() == t)\n",
        "print(f'accuracy={correct/len(T)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAA26pJqL_OA"
      },
      "source": [
        "That means that a pretty descent 97% of the class of all datapoints is correctly predicted by our model. (It's a pretty simple dataset after all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucAROT1pwWvN"
      },
      "source": [
        "## Section 2. Using Pytorch tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7QtcmTkPnkJ"
      },
      "source": [
        "In the case of Logistic Regression the gradient can be computed easily enough. However as models get more complicated, it becomes harder, messier and more error prone, to derive gradients analytically. Eventually it becomes a stumbling block to the entire programming effort. Obtaining the gradient of the loss function automaticall is one of the primary aims of Deep Learning frameworks such as `TensorFlow` or `Pytorch`. In this section we will revisit the Logistic Regression task but this time, pretending we are not able to compute the gradients. Let's begin by importing the `Pytorch` framework and casting the data into pytorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdzfegYlkXhO"
      },
      "source": [
        "import torch\n",
        "X = torch.tensor(iris.data,dtype=torch.float32)\n",
        "T = torch.tensor(iris.target,dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0gep7y-UA6E"
      },
      "source": [
        "Let us also initialize the weight matrix $W$ as a $C\\times F$ random `Pytorch` matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkfLuvi0UAUB"
      },
      "source": [
        "W = torch.randn(C,F,requires_grad=True,dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWK-5NbOUKwn"
      },
      "source": [
        "Only difference from before is this mysterious option flag `requires_grad` that is set to `True`. We will see what this does shortly.\n",
        "As before, we will focus on a single datapoint $\\mathbf{x},t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKfA7kdgTWvl"
      },
      "source": [
        "x=X[0]\n",
        "t=T[0]\n",
        "p=torch.exp(W @ x)\n",
        "y = p/sum(p)\n",
        "L = -torch.log(y[t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-g_r-TYUk0A"
      },
      "source": [
        "Again, notice how this is all identical to the equivalent `numpy` code above. The only difference being that we must call the tensor versions of `exp`, `log` etc because we are dealing with `Pytorch` tensors instead of `numpy` arrays. \n",
        "\n",
        "Now instead of going to the trouble of computing the gradient of $L$ with respect to $W$, we only have to execute a single (magic) line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9JVUXTOUyP7"
      },
      "source": [
        "L.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDpP3TtDU204"
      },
      "source": [
        "... and we're done! Of course you will ask, where is the gradient? We didn't define any `dW` variable! The answer is that `Pytorch`, very conveniently automatically calculates the gradient and hides it in a specially defined `.grad` attribute. So in our case the gradient can be found in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRq6NB9DVT96"
      },
      "source": [
        "print(W.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP1oGBkyVWS0"
      },
      "source": [
        "That's pretty neat right? What's more, every time we call the `.backward()` method on a value we just computed, `Pytorch` knows that it has to calculate gradients for all tensors that played a part in that computation and these gradients are accumulated additively. All of them? I hear you ask. Well actually that would be too computationally heavy. Instead, `Pytorch` does this only for tensors that have been defined with the `requires_grad` flag set to `True`.  So we just need to remember to set that flag for all model parameters, all tensors we would like to modify in order to minimize our loss function.\n",
        "\n",
        "All this basically means that the only thing we need to do is come up with the appropriate loss function for a dataset and problem, write down the code that computes it (known as the _forward step_) and `Pytorch` will calculate the gradients (known as the _backward step_).  I hope you take a moment to apreciate just how wonderfully elegant the scheme is!\n",
        "\n",
        "Putting everything in a loop around the dataset we now get code that looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXRfHah8k4qJ"
      },
      "source": [
        "Ls=[]\n",
        "\n",
        "W = torch.randn(C,F,requires_grad=True,dtype=torch.float32)\n",
        "alpha = 5e-4\n",
        "for e in range(1000):\n",
        "  Ltot = 0.0\n",
        "  for x,t in zip(X,T):\n",
        "    p=torch.exp(W @ x)\n",
        "    y = p/sum(p)\n",
        "    L = -torch.log(y[t])\n",
        "    L.backward() # This is where all the magic happens\n",
        "    Ltot += L.item()\n",
        "  W.data -= alpha*W.grad # gradient descent step\n",
        "  W.grad.data.zero_() # we need to remember to set the gradient to zero (pytorch)\n",
        "                      # cannot know when our loss function computation is complete\n",
        "  Ls.append(Ltot)\n",
        "plt.plot(Ls,'r-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFSFcKP4wfOv"
      },
      "source": [
        "## Section 3. Using Pytorch Neural Network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSfu0bExXnLt"
      },
      "source": [
        "Now we are ready to see some more `Pytorch` awesomeness. The developers of the framework have already included a wide range of classes that pre-define a lot of the building blocks of Deep Learning. We therefore rarely have to define loss functions from first principles (using `exp`, `log` and suchlike). Instead we can rely on ready made building blocks that can be efficiently connected to one and other. And once again, `Pytorch` will automatically compute gradients for us.\n",
        "\n",
        "For Logistic Regression the only ingredient we need is a `Linear` layer, that plays the role of the $W$ matrix we had before. The loss function can be defined using the `CrossEntropyLoss` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0NGNuz5csHC"
      },
      "source": [
        "W = torch.nn.Linear(F,C)\n",
        "loss = torch.nn.CrossEntropyLoss(reduction='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M3Ptv_Kcv_S"
      },
      "source": [
        "All `Pytorch` layers (including loss functions) can be used as functions on tensors, producing other tensors as results. So the function call \n",
        "\n",
        "```W(X)``` \n",
        "\n",
        "maps each of the 4D row vector elements of tensor `X` to a 3D output vector by multiplying by a weight matrix and addind a bias $b$ to the result. The loss function takes the output of `W` and the correct labels `T` and computes the total negative log likelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivVQBY_1d_3x"
      },
      "source": [
        "L = loss(W(X),T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJykEkPLeHSK"
      },
      "source": [
        "all in a single line of code! Now we can execute `backward()` function as before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5-kV0yUeOi2"
      },
      "source": [
        "L.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAWIFFAVePY-"
      },
      "source": [
        "Yet another convenience that `Pytorch` provides is the simplification of the gradient descent code. After all, there should be an easy way to abstract away what is always going to be the same line of code, i.e. taking the model parameter and subtracting its gradient (to be found in `.grad`) times the learning rate. And indeed that's what `Pytorch` offers. All we need to define is an optimizer object (in our case implementing Stochastic Gradient Descent, but there are many other variants) give it pointers to the parameters that should be optimized as well as a learning rate. All this is accomplished by "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec4-aabofUn3"
      },
      "source": [
        "optim = torch.optim.SGD(W.parameters(), lr=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M52xGuLgi_cq"
      },
      "source": [
        "Putting everything together gives us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZAyXzaxpRHM"
      },
      "source": [
        "X = torch.tensor(iris.data,dtype=torch.float32)\n",
        "T = torch.tensor(iris.target,dtype=torch.long)\n",
        "\n",
        "W = torch.nn.Linear(F,C)\n",
        "loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "optim = torch.optim.SGD(W.parameters(), lr=5e-4)\n",
        "\n",
        "Ls=[]\n",
        "for e in range(1000):\n",
        "  optim.zero_grad()\n",
        "  L = loss(W(X),T)\n",
        "  L.backward()\n",
        "  optim.step()\n",
        "  Ls.append(L.item())\n",
        "plt.plot(Ls,'r-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKQCGtn-fjA5"
      },
      "source": [
        "To compute the accuracy of the predictions all we need is to use the `.topk` method that returns the values and indices of the top $K$ elements along a particular dimension in a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja-NkZcUbb5B"
      },
      "source": [
        "_,T_pred = W(X).topk(1)\n",
        "acc=(T_pred.flatten()==T).sum(dtype=torch.float) / len(T)\n",
        "print(f'accuracy={acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4lTMoi1f7R0"
      },
      "source": [
        "\n",
        "I hope this lab has given you a taste of some of the things one can achieve with relatively little effort, using some of the existing `Python` machinery for Machine Learning. You can find lots more information in the super-helpful `Pytorch` doc pages. \n",
        "\n",
        "## Challenge:\n",
        "\n",
        "Can you apply your model to the __breast cancer__ and __wine__ datasets? They can be loaded using\n",
        "\n",
        "```sklearn.datasets.load_breast_cancer()```\n",
        "\n",
        "and\n",
        "\n",
        "```sklearn.datasets.load_wine()```\n",
        "\n",
        "What accuracies can you get?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx-gXaRbcbEV"
      },
      "source": [
        "## Appendix. Using sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU4Y9Kz9QGex"
      },
      "source": [
        "For completeness, here's how you would achieve the same task using existing ML software libraries (`sklearn`). These frameworks are great for employing off-the-shelf models, but are not very helpfull for building more complex models or researching completely novel machine learning architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt-r0bCFP8r_"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL21wN1SP8r1"
      },
      "source": [
        "estimator = LogisticRegression()\n",
        "estimator.fit(X,T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_URxrd2jK6J"
      },
      "source": [
        "T_pred = estimator.predict(X)\n",
        "acc = accuracy_score(T,T_pred)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}