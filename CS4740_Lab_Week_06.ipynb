{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_06.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrwJEl5k80fMBoAT+r9PFa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmNmOeK20Eat"
      },
      "source": [
        "# [CS4740 Labs] Lab 6: Deep Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMog9XKI3kmJ"
      },
      "source": [
        "##Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mClXvBar1zjf"
      },
      "source": [
        "One of the big successes of Deep Learning that sparked significant interest in the last few years is Deep Generative Models. These are deep neural architectures whose main goal is *analysis-by-synthesis*. In other words, they represent an attempt to analyse a complex dataset by synthesizing new datapoints that preserve the statistical properties of the original data. In achieving this, the deep generative models are able to extract meaningful information about the statistical distribution that gave rise to the original dataset, thereby providing us with some sort of *high-level understanding*. \n",
        "\n",
        "These Deep Generative architectures have been used to generate realistic data from a variety of sources, including among others, celebrity face images and artistic paintings!\n",
        "\n",
        "In this lab we will explore some of the properties of Deep Generative Models by running several experiments using simple versions of popular generative architectures. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258RXXL2Xqgi"
      },
      "source": [
        "##A very simple VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWhG9IP_Fm3a"
      },
      "source": [
        "Let's begin by exploring a very simple toy dimensionality reduction problem. To keep things easy to visualize, we will limit ourselves to two dimensions. The code below generates the well known spiral dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvZuCnvYXtsw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "def spiral(n_points, noise=1.5):\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * 780 * (2*np.pi)/360\n",
        "    dx = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    dy = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return np.hstack((dx,dy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbHH8JwzHEPB"
      },
      "source": [
        "Let's see what this looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0yOHYM7HG_5"
      },
      "source": [
        "X = spiral(n_points=1000)\n",
        "plt.plot(X[:,0],X[:,1],'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smFJ5iOGHNv4"
      },
      "source": [
        "What we immediately observe is that while the points are two-dimensional, the space they come from is only one-dimensional. Which means this dataset could be a good candidate for a Variational Auto-Encoder with a 1-dimensional encoding. We will be using the simple MLP class below for both encoder and decoder networks. The network layers are generated according to the input of the constructor. The `hidden` holds the sizes of all the hidden layers (if any). We also define the input and output sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w63j04_IXw4P"
      },
      "source": [
        "class MLPNet(nn.Module):\n",
        "    def __init__(self, input_size=2, hidden=[3], output_size=2):\n",
        "        super(MLPNet, self).__init__()\n",
        "        s0=input_size\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for h in hidden:\n",
        "            self.hidden_layers.append(nn.Sequential(nn.Linear(s0,h),nn.LeakyReLU()))\n",
        "            s0=h\n",
        "        self.classifier= nn.Linear(s0,output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for f in self.hidden_layers:\n",
        "            x = f(x)\n",
        "        return self.classifier(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qMJ-FfS0G7E"
      },
      "source": [
        "Let us experiment with an encoder network that takes maps two dimentional input to one dimensional output:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GlgYrL-0GJB"
      },
      "source": [
        "enc = MLPNet(input_size=2, hidden=[64,256,512,128], output_size=1)\n",
        "X = spiral(n_points=1000)\n",
        "x = torch.tensor(X,dtype=torch.float)\n",
        "\n",
        "z = enc(x)\n",
        "print(x.shape)\n",
        "print(z.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwRaBK6AAVhr"
      },
      "source": [
        "The decoder network will similarly map the 1D code back onto the 2D space of the original data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1e6TBAVAVh0"
      },
      "source": [
        "dec = MLPNet(input_size=1, hidden=[64,256,512,128], output_size=2)\n",
        "\n",
        "x_=dec(z)\n",
        "print(z.shape)\n",
        "print(x_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsderJfkBeN3"
      },
      "source": [
        "We are now ready to define the training code for encoder and decoder networks. The loss function we use is MSE because the autoencoder must reproduce its own input as closely as possible. The loss we compute has this form:\n",
        "\n",
        "`L = loss(x, dec(enc(x)))`\n",
        "\n",
        "which penalises the difference between the input datapoint and the reconstructed version of that datapoint as it passes through the bottleneck of the autoencoder. \n",
        "\n",
        "Every 1000 epochs we will produce a scatterplot of the reconstructed input data superimposed on the original data. If all goes well we should observe an overlap between the two. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlx8HtDPX7pi"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "N = 1000\n",
        "X = spiral(n_points=N)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x = torch.tensor(X,dtype=torch.float, device=device)\n",
        "num_of_epochs = 10000\n",
        "z_dim = 1\n",
        "\n",
        "enc = MLPNet(input_size=2, hidden=[128,256,128], output_size=z_dim).to(device)\n",
        "dec = MLPNet(input_size=z_dim, hidden=[128,256,128], output_size=2).to(device)\n",
        "\n",
        "optimizer = optim.Adam(chain(enc.parameters(),dec.parameters()), lr=0.001)\n",
        "\n",
        "loss = nn.MSELoss(reduction='mean')\n",
        "\n",
        "for e in range(num_of_epochs):\n",
        "    # z = enc(x)\n",
        "    # # z += 1e-2*torch.randn_like(z, device=device)\n",
        "    # x_ = dec(z)\n",
        "    # # L = loss(x, x_) + 1e-4*z.square().sum()\n",
        "    # # L = loss(x, x_)\n",
        "    L = loss(x, dec(enc(x)))\n",
        "    optimizer.zero_grad()\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if e%1000 ==0:\n",
        "        x_ = dec(enc(x))\n",
        "        x_ = x_.cpu().detach().numpy()\n",
        "        plt.figure()\n",
        "        plt.plot(X[:,0],X[:,1],'x')\n",
        "        plt.plot(x_[:,0],x_[:,1],'+')\n",
        "        plt.title(f\"{L.item()}\")\n",
        "        plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BplD8xN-bUyd"
      },
      "source": [
        "The auto-encoder seems able to reproduce the input with a very simple architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVj6s7gF3nhV"
      },
      "source": [
        "## A very simple GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpICUT6fhAUg"
      },
      "source": [
        "Before we move to more realistic applications, let us try a very simple (perhaps the most simple) GAN solution to the same spiral dataset problem. Similar to the auto-encoder, the GAN approach also uses two networks, a generator and a discriminator. In fact the generator can be identical to the decoder network of the auto-encoder in terms of input/output dimensionality. The discriminator is slightly different to the encoder network in that its output is a binary classification probability instead of an encoding. \n",
        "\n",
        "Given a datapoint $x$ and some randomly generated code $z$, we would like to optimize the following function with thespect to the parameters of discriminator $D$ and generator $G$:\n",
        "\n",
        "$$\n",
        "V(D,G)=\\log D(x)+\\log(1-D(G(z)))\n",
        "$$\n",
        "\n",
        "Remember from lectures, that we aim to maximize $V$ with respect to $D$ and minimize it with respect to $G$. However, as explained, the term $\\log(1-D(G(z)))$ has bad numerical properties as its gradient is very flat in the beggining. We therefore end up maximizing $\\log D(x)+\\log(1-D(G(z)))$ with respect to $D$, while also maximizing $\\log(D(G(z)))$ with respect to $G$. \n",
        "\n",
        "The  loss function for the discriminator looks like this:\n",
        "\n",
        "`L_dis = loss(disc(x), ones) + loss(disc(gen(z)),zeros)`\n",
        "\n",
        "and it is a straightforward classification error for classifier network `disc`. The first term is the loss incurred by correctly classifying the real data `x` as real while the second term is the loss of correctly classifying the synthetic data `gen(z)` as synthetic.\n",
        "\n",
        "The loss function for the generator looks like this:\n",
        "\n",
        "`loss(disc(gen(z)),ones)`\n",
        "\n",
        "and is essentially the mis-classification error of the discriminator classifying the synthetic datapoints `gen(z)` as real. We need to make this as big as possible, i.e. we need to make sure the generator is good enough to *fool* the discriminator.\n",
        "\n",
        "Because we have two separate loss functions, it makes sense to have two different optimizers for each of the two networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddj1udPyBXAa"
      },
      "source": [
        "N = 1000\n",
        "\n",
        "X = spiral(n_points=N)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x = torch.tensor(X,dtype=torch.float, device=device)\n",
        "num_of_epochs = 50000\n",
        "z_dim = 1\n",
        "\n",
        "disc = MLPNet(input_size=2, hidden=[128,256,128], output_size=1).to(device)\n",
        "gen = MLPNet(input_size=z_dim, hidden=[128,256,128], output_size=2).to(device)\n",
        "\n",
        "optim_d = optim.Adam(disc.parameters(), lr=0.001)\n",
        "optim_g = optim.Adam(gen.parameters(), lr=0.001)\n",
        "\n",
        "loss = nn.BCEWithLogitsLoss() \n",
        "\n",
        "zeros = torch.zeros(N,1,device=device)\n",
        "ones = torch.ones(N,1,device=device)\n",
        "\n",
        "for e in range(num_of_epochs):\n",
        "    z = torch.randn(N,z_dim, device=device) \n",
        "\n",
        "    L_dis = loss(disc(gen(z)),zeros) + loss(disc(x), ones)\n",
        "    optim_d.zero_grad()\n",
        "    L_dis.backward()\n",
        "    optim_d.step()\n",
        "\n",
        "    L_gen = loss(disc(gen(z)),ones)\n",
        "    optim_g.zero_grad()\n",
        "    L_gen.backward()\n",
        "    optim_g.step()\n",
        "\n",
        "    if e%1000 ==0:\n",
        "        x_gen = gen(z).cpu().detach().numpy()\n",
        "        plt.figure()\n",
        "        plt.plot(X[:,0],X[:,1],'x')\n",
        "        plt.plot(x_gen[:,0],x_gen[:,1],'+')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYLc_dA76mtD"
      },
      "source": [
        "## Auto-encoding Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlZS7nvwd4DA"
      },
      "source": [
        "We will now try the same VAE architecture on Fashion MNIST, a drop-in replacement for the well known MNIST dataset but this time using images of fashion items instead of hand-written digits. Let's load it up and have a peak:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANipNtoetde"
      },
      "source": [
        "dataset=torchvision.datasets.FashionMNIST('./data',download=True,transform=torchvision.transforms.ToTensor())\n",
        "dataloader=torch.utils.data.DataLoader(dataset, batch_size=64)\n",
        "\n",
        "x,_=next(iter(dataloader))\n",
        "grid = torchvision.utils.make_grid(x)\n",
        "plt.imshow(grid.permute([1,2,0]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBjY-kxGhXAP"
      },
      "source": [
        "Since these images are the usual 28x28 grayscale, we will need to reshape them as 784-dimensional vectors, exactly as we did to the MNIST digit images a few labs back. In order to evaluate the auto-encoder network we will write a special function that plots a few images together with their reconstructions via the auto-encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHu8zsuH7Rt_"
      },
      "source": [
        "def evaluate_autoencoder(x, enc, dec, title=\"\"):\n",
        "    x = x.view(-1,784).to(device)\n",
        "    x_ = dec(enc(x))\n",
        "    x = x.cpu().detach()\n",
        "    x = x.view(-1,1,28,28)\n",
        "    x_ = x_.cpu().detach()\n",
        "    x_ = x_.view(-1,1,28,28)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_ = torchvision.utils.make_grid(x_)\n",
        "    plt.figure()\n",
        "    f,ax = plt.subplots(1,2)\n",
        "    ax[0].imshow(grid.permute([1,2,0]))\n",
        "    ax[0].set_title('input')\n",
        "    ax[1].imshow(grid_.permute([1,2,0]))\n",
        "    ax[1].set_title('reconstruction')\n",
        "    f.suptitle(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G7Yl0g5LUOs"
      },
      "source": [
        "Now we are ready to write the auto-encoder training code. It's very similar to the simple VAE code above (the same network architecture is used) but with some differences to do with image data pre-processing and dimension reshaping. We also add a noise term and a penalty for the L2 norm of the embedding that has the same effect as the KL divergence term. This brings the network closer to the true probabilistic VAE we saw in lectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgIQ1ds9V5vG"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "N = 1000\n",
        "X = spiral(n_points=N)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x = torch.tensor(X,dtype=torch.float, device=device)\n",
        "num_of_epochs = 50\n",
        "z_dim = 10\n",
        "\n",
        "enc = MLPNet(input_size=784, hidden=[128,256,128], output_size=z_dim).to(device)\n",
        "dec = MLPNet(input_size=z_dim, hidden=[128,256,128], output_size=784).to(device)\n",
        "\n",
        "optimizer = optim.Adam(chain(enc.parameters(),dec.parameters()), lr=0.001)\n",
        "\n",
        "loss = nn.MSELoss(reduction='mean')\n",
        "\n",
        "noise_scale =1e-2\n",
        "l2penalty = 1e-3\n",
        "\n",
        "for e in range(num_of_epochs):\n",
        "    x, _ = next(iter(dataloader))\n",
        "    evaluate_autoencoder(x, enc, dec, title=f\"{e+1}/{num_of_epochs}\")\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        x = x.view(-1,784)\n",
        "\n",
        "        z = enc(x)\n",
        "        z += noise_scale*torch.randn_like(z, device=device)\n",
        "        x_ = dec(z)\n",
        "        L = loss(x, x_) + l2penalty*z.square().sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        L.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l2JSl9Ujn_7"
      },
      "source": [
        "The results of the VAE look a bit fuzzy, just as we discussed in the lectures. The best theory about why this\n",
        "happens is the L2 norm loss function for images. This tends to smooth out the details, putting more emphasis on global features. If you look closely, you will see that some of the detail on the clothes (t-shirt print etc) is blurred our."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMMsxesI5uZU"
      },
      "source": [
        "Let's try interpolating between two datapoints. The idea is, we take two random images from the dataset, convert them to z-encodings and then linearly interpolate between the encodings. We then decode each of these linearly interpolated z-encodings back to image space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9exa6nP8jHCq"
      },
      "source": [
        "x0 = dataset[10][0].view(-1,784).to(device)\n",
        "x1 = dataset[40][0].view(-1,784).to(device)\n",
        "\n",
        "z0 = enc(x0)\n",
        "z1 = enc(x1)\n",
        "t = torch.linspace(0,1,16,device=device).view(-1,1)\n",
        "z_interp=torch.lerp(z0,z1,t)\n",
        "x_interp = dec(z_interp).cpu().detach().view(-1,1,28,28)\n",
        "\n",
        "grid = torchvision.utils.make_grid(x_interp,nrow=4)\n",
        "plt.figure()\n",
        "plt.imshow(grid.permute([1,2,0]))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4DIQKWkIcQZ"
      },
      "source": [
        "## Challenges:\n",
        "\n",
        "* What can we say about the effect of the `l2penalty` and `noise_scale` term? Run a few experiments to find out.\n",
        "* Can you use the code supplied in order to fit a GAN model to the Fashion MNIST data? \n",
        "* **Advanced:** If you would like to try your hand on a cutting edge research problem, think about how we could use combine VAEs with GANs? Can we have a VAE that has the standard reconstruction error, combined with an adversarial discriminator loss function? Does this undo  some of the blurriness of standard VAEs?\n"
      ]
    }
  ]
}
