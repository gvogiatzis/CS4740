{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDFvKJta3Py4X1MBkWhU9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1eebe9516234472a95a8f80b70c4b3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a01d406aa91b4959b72aad1d7f50932c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fc5e5dcc0a674aad8651827de8c934a5",
              "IPY_MODEL_a871514662034589b77da6c90f8fc4a9"
            ]
          }
        },
        "a01d406aa91b4959b72aad1d7f50932c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc5e5dcc0a674aad8651827de8c934a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_20913fb0d120414eb76f4264f81d0adb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29bf6725d8824380ab3e1c0dfe4e9f46"
          }
        },
        "a871514662034589b77da6c90f8fc4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9c6a464894c4387a19d62ae71093eb6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:09&lt;00:00, 17664363.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f43cb4209cba4a7f9d396f13c6654640"
          }
        },
        "20913fb0d120414eb76f4264f81d0adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29bf6725d8824380ab3e1c0dfe4e9f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9c6a464894c4387a19d62ae71093eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f43cb4209cba4a7f9d396f13c6654640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85155b43cac64c4ba438e4d9bf48a71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_583a90f0284b4241b0f8b54001372b68",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3552085cf9c24391ab4e629808af11b3",
              "IPY_MODEL_5b479cb13a774746a69bb8d12112f839"
            ]
          }
        },
        "583a90f0284b4241b0f8b54001372b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3552085cf9c24391ab4e629808af11b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6a9dc68c57c49a3a2cf66593b322d97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5de1a21dfe114398991d765ab990c45d"
          }
        },
        "5b479cb13a774746a69bb8d12112f839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_56e91bdf03174f8894c9657c1e8c7529",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:06&lt;00:00, 28325300.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_beb21987ed1c487e896e88108234fd88"
          }
        },
        "e6a9dc68c57c49a3a2cf66593b322d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5de1a21dfe114398991d765ab990c45d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56e91bdf03174f8894c9657c1e8c7529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "beb21987ed1c487e896e88108234fd88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYtZOri9fuqF"
      },
      "source": [
        "#[CS4740 Labs] Week 4:  Convolutional Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twt8uVGzXsgo"
      },
      "source": [
        "##Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3WvMcjXzpe"
      },
      "source": [
        "In this lab we will extend the classifier we have seen previously, to CIFAR10, a more challenging dataset of real (although very low-res) images. The dataset is split into ten classes (see below) and there is significant variability within each class.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1bn4UqHwT0v7Twk01KS9d_jVlOQpeMaNc'/>\n",
        "<figcaption>CIFAR10 images</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "To classify this dataset correctly we will need to upgrade our network to use convolutional layers. And because convolutions are very computationally demanding, we will see how to offload some of those computations to the GPU, dramatically reducing training times. \n",
        "\n",
        "\n",
        "First, let us load up the CIFAR10 dataset, for both training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SudY_3wf3Qn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "1eebe9516234472a95a8f80b70c4b3f0",
            "a01d406aa91b4959b72aad1d7f50932c",
            "fc5e5dcc0a674aad8651827de8c934a5",
            "a871514662034589b77da6c90f8fc4a9",
            "20913fb0d120414eb76f4264f81d0adb",
            "29bf6725d8824380ab3e1c0dfe4e9f46",
            "f9c6a464894c4387a19d62ae71093eb6",
            "f43cb4209cba4a7f9d396f13c6654640"
          ]
        },
        "outputId": "7632fd23-ce5d-4846-c82d-bcf9898de9d8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10('.', train=True, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10('.', train=False, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=100, shuffle=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eebe9516234472a95a8f80b70c4b3f0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhzjyj-0WMGF"
      },
      "source": [
        "Let's examine one of the training datapoints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "bgkv-SbiWghN",
        "outputId": "3df6aaf1-6f35-462b-d16c-fa11763a74ca"
      },
      "source": [
        "img, target = train_dataset[30]\n",
        "print(img.shape)\n",
        "plt.imshow(img.permute([1,2,0]))\n",
        "print(train_dataset.classes[target])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "airplane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb+UlEQVR4nO2da4ydZ3Xv/2vvPeO52uPx2ONrfI8vpElIJyaUNKIXOAFVCogjTvIBRSrCVVWkIrUfIioVjtQP9OgA4kNFZZqooeUAKQESncJpQoQaUkrImATHl1wcx47tOr7f7bnsvdf5sLelSfT814zfmdkT8vx/kuU9z5pnv2s/+13vu+f577WWuTuEEO9+SnPtgBCiNSjYhcgEBbsQmaBgFyITFOxCZIKCXYhMqExnspndDeBrAMoA/sHdvxT9fldXp/fNn5+0ObgEaGbX79t1z5jKRGKM5Mvg+SwwFpVE2VpFz1bkJU8KOWBhqbfwG0oI3CguRs/sk0YxwZbx/IWLuHp1JLlahYPdzMoA/g7AhwAcAfCcmT3u7nvZnL758/GZ++9N2qq1Oj1WpVJO+wA+p1wu9qGlVOLzWCDVarVCzxfZoueMKJfTaxUFGZsDFLvQRscbHx+/7jlAvFZFqNX5+tYLXpDqdX4+FrnIVatVamPnxze//RidM50V3AZgv7sfcPcxAN8BcM80nk8IMYtMJ9hXADg84ecjzTEhxDuQWd+gM7PtZjZsZsNXrl6d7cMJIQjTCfajAFZN+Hllc+wtuPsOdx9y96Guzs5pHE4IMR2mE+zPAdhoZmvNrB3AvQAenxm3hBAzTeHdeHevmtlnAfwbGtLbQ+6+J5wDR5XsIka7z+7pXc5yie8UlwJbtMMc7agWmVN0N7voPEa04x7ZiioGbE2iXelWrlWpxF8zyPk22bGitYrOEbaOM63WTEtnd/cfAfjRdJ5DCNEa9A06ITJBwS5EJijYhcgEBbsQmaBgFyITprUbP5MUSXSwQF4rml0V+THTxTmLJn4USdYpKmsVlRUrlfSpxcan40cE8zFKdomyzYpIaECx8yo6P9g6Rm+z7uxCZIKCXYhMULALkQkKdiEyQcEuRCa0fDe+yG4x28ksWVSjq1jCRbTDz6pgRTutZVJSC0BYlyzyMdr1NSNrFfhYqRTb+Q/VCWKKXleoTgQ+esEdckao8gTzipalYmtSrBRXEEcFnk0I8RuIgl2ITFCwC5EJCnYhMkHBLkQmKNiFyISWSm9mBiOdWqJWSKy7S/Sl/0DwQiW4xkV+1Mi0KMXEgvyNepRwETxn2QNpqJSWmqwciEbBwbwe+BiplOSFR685KP2GUrzKgR9kPEqsqRaTdMsFa9CxpJwidQNDCZtahBDvKhTsQmSCgl2ITFCwC5EJCnYhMkHBLkQmTEt6M7ODAC4CqAGouvvQpJOIPBFWd2OSRlQDzfl1rDMQ5iI5abREvAzkqUqN26qBPlgLZJeuCm+QeQXnk+N1IskBgNWDa37hdk3peXWv8hkkYw8AzPl6WAEfo/PNgvczfMUFSxRSH2e45uFM6Oy/5+6nZuB5hBCziD7GC5EJ0w12B/CEme00s+0z4ZAQYnaY7sf4O939qJktAfCkmb3k7k9P/IXmRWA7AMyf3zvNwwkhijKtO7u7H23+fwLADwBsS/zODncfcveh7i6+sSSEmF0KB7uZdZtZ77XHAD4MYPdMOSaEmFmm8zF+EMAPmrJBBcD/cff/F01wFGt1w8SaUiD9VANxZbQcZbZxG5PewhqVgYwzOjJKbaWOLmqrdvJPSP2V9uT4xasX6ZzLJKsQAKzEpbJ5gRDVPp5+zo4xLqHVmLQJoB7YokxFI9JnJcgcHC9U6HESqSzKfiTpflGLKppFF7WM4i7EuPsBALcUnS+EaC2S3oTIBAW7EJmgYBciExTsQmSCgl2ITGhtrzefpNAfgc2JpI7xMn9prBgiAFgwr72tLTleHQ8yudr59bQ9kLyiHnFHD+2jtt6L48nxwRVL6Zx6fwe1VaMChkFmYZWYSvP4+not7XvDD77GUdKeEQm2FJyGcTYfp2iWGpsV9iRktsB13dmFyAQFuxCZoGAXIhMU7EJkgoJdiExocfunuKUNo0QSE8LaY0EyQ6kS7LgH17+lvf3J8bFxvot8+vIFaqu0z6O2EnjNuCV9fN6ZN9MVwkavLKBzOoLd+PFqUOePWoAySVzx+lgwhx/rcrDzfzFobcWmtfG3DJVgUz3acS9qYxTajQ/QnV2ITFCwC5EJCnYhMkHBLkQmKNiFyAQFuxCZ0NpEGBiV0SLYnOi5PJBxyoFs0XaVJ1yc2vNacnzxMp5k0hUk1owGdfKq1aCW2PxBarP1C5PjV/oG6JyFC3iJ7+qlE9TWcfkStdVf2Z8cLx8+TOeU+/g6Vm5cT23Wl667BwAjRMKMz8KZbbsEIGxVxiy1Gpdfi6A7uxCZoGAXIhMU7EJkgoJdiExQsAuRCQp2ITJhUunNzB4C8EcATrj7Tc2xfgDfBbAGwEEAn3T3s5MfzgFagy6odUYuSUba5jRs/Pmcq2uoBxLJob2vJsdPPr+Hzrnhzt+itmp/D7VdDi7DlQo3niHZVS8dOEnndL3JF2TzptXU1j52htpGT6fXanCUZ9hd2JOeAwB+4Ty19d/O1/jsgnSrrJGobh1pGQXEcm+ERzJagQw23v6Jz5mK5/8I4O63jT0A4Cl33wjgqebPQoh3MJMGe7Pf+tsv4fcAeLj5+GEAH5thv4QQM0zRv9kH3f1Y8/GbaHR0FUK8g5n2Bp03SnDQvxTMbLuZDZvZ8JUrV6d7OCFEQYoG+3EzWwYAzf/pF6jdfYe7D7n7UFcX7ysuhJhdigb74wDubz6+H8BjM+OOEGK2mIr09m0AHwQwYGZHAHwBwJcAPGJmnwZwCMAnp3Iwg6Fs6etLVI/PiK1EihoCQKlgsb5aJy/muPkDdyTHx44e5ccKiiHa2Ai1uadbTQHA+s23UNvSG9IFPY+cuEjnvHaYZ7a9eZ5XZmyv9FHb/C23JccXL+TruxHcx+d2/pzaUOKyVoW02LKgDZnVi7V/KirLgWRoRufpOCtyGrg+abC7+33E9AeTzRVCvHPQN+iEyAQFuxCZoGAXIhMU7EJkgoJdiExobcFJAyqkz1rUC4sWnAxkrYhSifebi/rHvUq+Adi76WY6Z+uGVdR2+vBBart0iGepHT/bTW0333ZTcry9ax+ds2I5Lzi5eMkKauvm6hVO7k9Lh+UeXhyyc2W6lx4AoIu/L5eqPGuvTKSorkCjGg+yCmm22WS2IEMTBaQ+LvMF2aPXfRQhxG8kCnYhMkHBLkQmKNiFyAQFuxCZoGAXIhNaKr0ZjGbyRBlDxrth8TlBxlAk87EsKQA4fiotvf3wZ7+kc+54H5dc7tiWzgwDgLUruDy4/9BBajv/i3Tm2Lrly+icG5ZwW/9CXoOgzBPz0DuQfm0evGev7OWFO8dG+alabuPvWRWjaT9KUbHS4FwMzqvwHK4FmZZEsism8wUSNrUIId5VKNiFyAQFuxCZoGAXIhMU7EJkQkt34x2OGmmDUyYJMtdmpojSB6Kd0Sgn4fXXX6e21UvXJ8d7+7bSOTv37qe2w6cuUNutt/Pn3LphHbVVr6Z3n1/ef4TOOTrvNLX19/HEle5gp37+AGltNcbLiZ9+g9fy6w12wceCHegRUsDQo5ZLQaemaIe8SDJXg7QvxZ5PiTBCZI+CXYhMULALkQkKdiEyQcEuRCYo2IXIhKm0f3oIwB8BOOHuNzXHvgjgMwCuFUr7vLv/aCoHZMJAdXSMzmlrS8s/lXLgfqCseFBjrK9/gNrWbF6THG9bwOu0bdzMa9ChjbdCujrCJaqdz/CEkRtvXJsc37BlI/cDfO1HLvEWVcdPX6K2E6fOJscX9xBJDkDbovnUdun8OWrzcV6DrkLuZzU+JZTlIjmMycoAYIH0NjaWXv8wsYb4GMrRge0a/wjg7sT4V9391ua/KQW6EGLumDTY3f1pAGda4IsQYhaZzt/snzWzXWb2kJktnDGPhBCzQtFg/zqA9QBuBXAMwJfZL5rZdjMbNrPhy5evFDycEGK6FAp2dz/u7jV3rwP4BoBtwe/ucPchdx/q7u4q6qcQYpoUCnYzm1jH6OMAds+MO0KI2WIq0tu3AXwQwICZHQHwBQAfNLNb0UhHOwjgT6ZysJIZOtvStdXGg0w0lqVmdZ6RNeZcTupdspjabrnjA9S2+8Tl5PiJo8fpnLvWraG27kX8k05PuYPaXl2+hNpeO5zOHHtx1yk6p38pr0G3ZiWXIldVeNbblfPpN+3RJ/hatfXy17xxkLeoWmDnqa1eG0+O12pBAb2gBl2pzKXIqD4dnNcUbK+kz+O6cymvXr/+1meTBru735cYfvC6jySEmFP0DTohMkHBLkQmKNiFyAQFuxCZoGAXIhNaWnAScNSRlmSqHVy2GCdeVss8dWn5wnT2FwAMLr2J2p569jC1HT75X8nxD67m2Vrd9XT2FwBcmZeWhQDAuvh1eP06LoctX7koOX7iAs+i2/sqL/T4438/QW1bNnCpbM2Spcnxl19MryEAnD7DT8e2P1xNbUsWHKK2xd1pqaxsXEKrWVpiBQDzICMuyDmL2l5VKmlZrl4PYqLOzx2G7uxCZIKCXYhMULALkQkKdiEyQcEuRCYo2IXIhNb2enNgnGTytBFJDgC6q2mZobSb91HrWMuzjH68n8suF2vd1PaRJf3J8StPfJ/OObqR92Xbet8nqG10nPvfPY8Xqlw8kM6kWxHUvdx0I8+ie+Z5Lms99pNfU9va1X3J8W3v5xLaf/7kGLUdPLyc2va+xouibFt3MTm+PJDrqhXeg682zjP9yiWeSVcPsjDN0vOC2pZh4UuG7uxCZIKCXYhMULALkQkKdiEyQcEuRCa0OBEGKJNNxIWneB2xtr2vJ8c7971I55z92S5q61pzM7X97v/4JLWtHUgnfpz099E5PWt426UFbYPU1t6zgNqujvCeHftfSu+Ql4J3etkyXpPvE9v4Nv7qQT7v77+3Mzne18WThj7xx1uo7adPnaa2o4f4Oh7pTB9vYD5PTCnX+a56ucx31R1RTyl+X63V0kpUkR33CN3ZhcgEBbsQmaBgFyITFOxCZIKCXYhMULALkQlTaf+0CsA3AQyi0e5ph7t/zcz6AXwXwBo0WkB90t15wTUA5o7SeFryOLlnL53Xv3NfcrzDuDQxWOKJNQP7nqO2cw/zGnRX7rs3Ob7hEx+nc2r9XJ4aOZ1O0gCAXww/QW3/9sMfUtvzw2nJq62Ny0mrV/HklPfcuJnaNm37LWr78O3pdk3//N1f0jnL5m+ltv/2h+madgDwr+fT0iwA9C9P+3/yAk946hjh98BFK49QW7XOa9fV61xyrNdHk+PO+p4BqNeJXAceE1O5s1cB/IW7bwVwB4A/M7OtAB4A8JS7bwTwVPNnIcQ7lEmD3d2Pufuvmo8vAtgHYAWAewA83Py1hwF8bLacFEJMn+v6m93M1gB4L4BnAQy6+7UE5DfR+JgvhHiHMuVgN7MeAI8C+Jy7vyW73xvf60v+sWBm281s2MyGL13htcuFELPLlILdGqU0HgXwLXe/VpbluJkta9qXAUh2E3D3He4+5O5DPV28yocQYnaZNNit0WH+QQD73P0rE0yPA7i/+fh+AI/NvHtCiJliKllvHwDwKQAvmtkLzbHPA/gSgEfM7NMADgHg6WIToO1zBtL13QDgwur0dkD1HM9AWnD1ErX117nkVTrA/9R445EfJcevLOCy0OvjvD7az3/8r9S266Xnqa27g8s4g4vS2XKXLnBZ6OU9u6nt+V1pKQ8A7FF+rxhYlK4ZV+nk2Xwv/scb1Pah3/sdavvIh3kG25uXTybHD7/KWyv119J1/ACgc4BLmG1tPJxKQdZbnUhsdVKvEQgy4oJEuUmD3d2fAWgTqz+YbL4Q4p2BvkEnRCYo2IXIBAW7EJmgYBciExTsQmRCa9s/mWG8nJYujrZzuWOfpa9J713FJa/Nl7nkdeYcT847W+WZRrsOvZYcf+VvvkDnnKjzVlO9fVxCu/22IWq7cT1vKdXRkS6KOTbKZcrLl7ksd+48X8ezZ3ibpNMn0wUiL189RefMC7LNjhzgLa/6B3lrqL7edBHIlXdtoHOW9d9ObfPKPEPw9Zd/Tm1j47xgZqmU9rFe49KbMX2MK4q6swuRCwp2ITJBwS5EJijYhcgEBbsQmaBgFyITWiu9uWOsOp60vXLoEJ2368CB5PjBBQvpnM0LFlFbR9oFAMChCzxb7kw5LYUs6uF+3H7rb1Pbls28t1l/T7pgIwBU61xGqxG5pqsrLckBQE8Plz2XLg2yteo8xapWS8tJIyPp4ooAcOJUOkMNAN449Aq1XQwyHFesWZ8c7+9fQues3bqG2pYPvIfaunt5NuXOXzxNbVWyJHWWIYpg7YOsN93ZhcgEBbsQmaBgFyITFOxCZIKCXYhMaO1uPABHerd4y5ZNdF7HvPbk+M4D6cQUAPiPY7yNU5/xl73gBp5cc/OmtcnxretW0TkDfby2XqXGt07HguQUb7/+azRrFzSZrVbn0kW5zHeLS+Vycry7J/1eAsDqHp7Q0ruQqxOHDv8Xtb2yezg5fukiT4aqjvHdfXvPTdS2YfNt1DZW5bXrdv7ip8nx8Sqvh1gicRShO7sQmaBgFyITFOxCZIKCXYhMULALkQkKdiEyYVLpzcxWAfgmGi2ZHcAOd/+amX0RwGcAXMte+Ly7p/sjXcMdtVpaylmwgEsrt78vnUwyuHIxnXP04BFqW9zLk2TWrr+B2roWER8jCWqcS1dXL/H6bmPVdCIJAFg7l3HmzUvXamtr43NKpeiaz+VB1oEIAOr165eGPLj39PXyen3zt/Bz54030hLsqy/w9lqnjvB6cSNneSLPLb/9fmq76ZY7+XOOpmXWnc8+Q+cYaQ0VvCVT0tmrAP7C3X9lZr0AdprZk03bV939f0/hOYQQc8xUer0dA3Cs+fiime0DsGK2HRNCzCzX9Te7ma0B8F4AzzaHPmtmu8zsITPjSd1CiDlnysFuZj0AHgXwOXe/AODrANYDuBWNO/+XybztZjZsZsNXrvKv/wkhZpcpBbuZtaER6N9y9+8DgLsfd/eau9cBfAPAttRcd9/h7kPuPtTV2TlTfgshrpNJg93MDMCDAPa5+1cmjC+b8GsfB7B75t0TQswUU9mN/wCATwF40cxeaI59HsB9ZnYrGrv9BwH8yeRP5TTrrUpqlgGAVdOCwprly5LjALB6GZfQ2iu85tq8Es8Aq9aI7FJKZ3gBQIX26QHQyTPAanV+HS4Fb1ulcv2JjB5oaF4P/Df+ulkfouhYVfI+N+DvS6XM12rtinRG4qLuBXTOwUNHqe1nT/6Q2l478BK1bbvzLmrbuCmd8Xn2DK/J9/q+XcTC13Aqu/HPIP3OxZq6EOIdhb5BJ0QmKNiFyAQFuxCZoGAXIhMU7EJkQksLTjZIX19KJe5KWyUtUUVCTS2QhcaMz6wH0lCF+FgK2iBVo0KPxq+17e28XVMleG2s/dN4kH3X0cGPFd0PgpeGUolJb3zO6Bhva1Wu8Ky9ODMv7WRXD/+C15at6cKiAHDy3AVqO/rmy9T2L9/aQ22bNqVbSm1Yt5rOKdNMxSADk1qEEO8qFOxCZIKCXYhMULALkQkKdiEyQcEuRCa0WHozlJCWjSptPAOMXZOiDKpKIK+ZcxnKgyw1JjUFh0KYyBXJJM5ttaCYI+vbZlH2XQCTrho2Pq9IwclyJciiI1IeANSil0bkzfFaoBsGa9U/wIuVLuznxZrOnjtHbccP7U+Oj57nWW8dHUQ6DN4U3dmFyAQFuxCZoGAXIhMU7EJkgoJdiExQsAuRCa3PeiOyBpOMIqIeZaHiFckTBfqeFfG94UYgDxaUymL/01SDvnJFKeJ/JOXxLK+4yCZb43JQpLKIbAjEGZOL+nlfwvm9acnu8uV0DzgAGBkZSY5H55Tu7EJkgoJdiExQsAuRCQp2ITJBwS5EJky6G29mHQCeBjCv+fvfc/cvmNlaAN8BsAjATgCfcndeRKwJ2y2Mdm/ZnGjnsYgPQLybzRJQzPnubfS6irzmyWDKwGzs/Bd5baEfQRutorDj1aO6gVWursRrxc+dsTGefMXUkKLnAGMqd/ZRAL/v7reg0Z75bjO7A8DfAviqu28AcBbAp2fUMyHEjDJpsHuDS80f25r/HMDvA/hec/xhAB+bFQ+FEDPCVPuzl5sdXE8AeBLAawDOufu1zx9HAKyYHReFEDPBlILd3WvufiuAlQC2Adg81QOY2XYzGzaz4StXrxZ0UwgxXa5rN97dzwH4KYD3A+gzs2sbfCsBJJtau/sOdx9y96GuTl6YXwgxu0wa7Ga22Mz6mo87AXwIwD40gv6/N3/tfgCPzZaTQojpM5VEmGUAHjazMhoXh0fc/f+a2V4A3zGzvwHwPIAHJ3siR7GkkXI5LcnMhpwU+WfseAX9iPxnbZwme042r6gEWBTWbipKuim3zaO2qGRckXMqVLWi+n+BI9H7WQ9q+bGsrba2qOVVmui9nDTY3X0XgPcmxg+g8fe7EOI3AH2DTohMULALkQkKdiEyQcEuRCYo2IXIBJvpzJrwYGYnARxq/jgA4FTLDs6RH29FfryV3zQ/Vrt7suBdS4P9LQc2G3b3oTk5uPyQHxn6oY/xQmSCgl2ITJjLYN8xh8eeiPx4K/Ljrbxr/Jizv9mFEK1FH+OFyIQ5CXYzu9vMXjaz/Wb2wFz40PTjoJm9aGYvmNlwC4/7kJmdMLPdE8b6zexJM3u1+X+6J9Ds+/FFMzvaXJMXzOyjLfBjlZn91Mz2mtkeM/vz5nhL1yTwo6VrYmYdZvZLM/t104//2Rxfa2bPNuPmu2bWfl1P7O4t/QegjEZZq3UA2gH8GsDWVvvR9OUggIE5OO5dAG4DsHvC2P8C8EDz8QMA/naO/PgigL9s8XosA3Bb83EvgFcAbG31mgR+tHRNABiAnubjNgDPArgDwCMA7m2O/z2AP72e552LO/s2APvd/YA3Sk9/B8A9c+DHnOHuTwM487bhe9Ao3Am0qIAn8aPluPsxd/9V8/FFNIqjrECL1yTwo6V4gxkv8joXwb4CwOEJP89lsUoH8ISZ7TSz7XPkwzUG3f1Y8/GbAAbn0JfPmtmu5sf8Wf9zYiJmtgaN+gnPYg7X5G1+AC1ek9ko8pr7Bt2d7n4bgI8A+DMzu2uuHQIaV3ZM0nV6Fvk6gPVo9Ag4BuDLrTqwmfUAeBTA59z9wkRbK9ck4UfL18SnUeSVMRfBfhTAqgk/02KVs427H23+fwLADzC3lXeOm9kyAGj+f2IunHD3480TrQ7gG2jRmphZGxoB9i13/35zuOVrkvJjrtakeezrLvLKmItgfw7AxubOYjuAewE83monzKzbzHqvPQbwYQC741mzyuNoFO4E5rCA57XgavJxtGBNrFE47UEA+9z9KxNMLV0T5ker12TWiry2aofxbbuNH0Vjp/M1AH81Rz6sQ0MJ+DWAPa30A8C30fg4OI7G316fRqNn3lMAXgXwEwD9c+THPwF4EcAuNIJtWQv8uBONj+i7ALzQ/PfRVq9J4EdL1wTAzWgUcd2FxoXlryecs78EsB/AvwCYdz3Pq2/QCZEJuW/QCZENCnYhMkHBLkQmKNiFyAQFuxCZoGAXIhMU7EJkgoJdiEz4/wHPYzcac3DYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFhLhDj-X1dH"
      },
      "source": [
        "The images are 32 by 32 and contain 3 color channels which is what we would expect for color images. This is very low res, however if a human is able to correctly classify these images, we should be able to program a neural network. We will be using a convolutional neural network architecture for this task, but first, let us understand how pytorch performs convolutions. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oUMLtlgXkoO"
      },
      "source": [
        "## A simple convolution network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JQzvvhBX4pW"
      },
      "source": [
        "We define a convolutional layer with the Conv2d class. The constructor takes the input and output number of channels as well as the size of the convolution kernel. Consider the following convolution layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YOf3hAhTiPG"
      },
      "source": [
        "conv = nn.Conv2d(3, 16, 5)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CNxyrnehzJA"
      },
      "source": [
        "This layer receives as input a tensor with 3 channels, and outputs a tensor with 16 channels. The size of the kernel is 5x5. This means that if it is applied on a 3x32x32 tensor it will produce a 16x28x28 tensor output. The 28 comes from the fact that we apply a convolution with a 5-element kernel. A 5-element kernel can slide into 28 (32-5+1) different positions in a 32 element vector.\n",
        "\n",
        "Also note that pytorch layers expect to be applied to batches of tensors. This makes our lives much easier when we come to training but when applying the layer to a single tensor we must remember to insert a dummy singleton dimension in the start (i.e. 1x3x32x32). We can do this with the unsqueeze command.\n",
        "\n",
        "Let's check if the layer gives the output we predict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYz2fT1kTqW6",
        "outputId": "7f86918e-f73b-4a2a-bc07-3309efc681f9"
      },
      "source": [
        "img, target = train_dataset[30]\n",
        "img = img.unsqueeze(0) # adding a dummy singleton dimension\n",
        "out = conv(img)\n",
        "print(out.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpviC6TTlhSk"
      },
      "source": [
        "So the convolution layer behaves as expected. Now let's define a conv net architecture. There are litteraly infinite choices but we will keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHGDn28KiBIg"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
        "        self.conv2 = nn.Conv2d(16, 64, 5)\n",
        "        self.conv3 = nn.Conv2d(64, 256, 5)\n",
        "        self.conv4 = nn.Conv2d(256, 128, 1)\n",
        "        self.conv5 = nn.Conv2d(128, 10, 1)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv5(x)\n",
        "        return x.view(-1,10)\n",
        "\n",
        "convnet = ConvNet()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGGqgx4LS-6U"
      },
      "source": [
        "The network consists of a sequence of convolusions, relu's and maxpooling layers. Let's follow it step-by-step, as it processes a 3x32x32 tensor.\n",
        "\n",
        "Firstly the network applies a Conv2d(3,16,5) convolution to the 3x32x32 tensor, thereby producing a 16x28x28 output. This is then maxpooled with a 2x2 window. This always has the effect of halving the width and height, giving us a 16x14x14 tensor. \n",
        "\n",
        "This is then colvolved with a Conv2d(16, 64, 5) layer which produces a 64x10x10 output which when maxpooled gives 64x5x5.\n",
        "\n",
        "The network then applies a Conv2d(64, 256, 5) layer which produces a tensor with 256 channels but a width and height of 1. I.e. 256x1x1. This makes sense because the 5x5 kernel can only fit in one way to a 5x5 matrix. The 256x1x1 tensor is in fact just a 256-dimensional vector. \n",
        "\n",
        "We then see a Conv2d(256, 128, 1), which essentially is a fully connected linear layer masquerading as a convolution! When applied to the 256-dimensional vector it has the effect of multiplying that vector with a 256x128 matrix, giving a 128-dimensional vector as output. \n",
        "\n",
        "A convolution that works out as a matrix multiplication may be unintuitive at first but you can convince yourself by working out a simple example. E.g. apply a Conv2d(3, 7, 1) convolution to a 3x1x1 vector to see how you would get a 7x1x1 vector, thereby multiplying with a 3x7 matrix.  It is a useful trick for several reasons, the main one being that we avoid doing unnecessary reshaping of the output tensors. Also, by staying within the convolution world, we can convert any classification network, to a `sliding-window' classifier that can produce classification scores accross all regions of a larger image. \n",
        "\n",
        "Finally the network applies a Conv2d(128, 10, 1) layer to the 128x1x1 tensor (again essentially a FC layer), leading to a 10-dimensional vector. This vector is applied as logits which, when fed to a soft-max will give us probabilities for each class.\n",
        "\n",
        "A word of caution here: in the previous lab, we used a log_softmax output layer and a NLLLoss loss function. Here, we just use a linear output layer (i.e. no processing via soft-max or logs) which forces us to use a CrossEntropyLoss loss function. Either approach is fine. Just make sure you never mix log_softmax with CrossEntropyLoss etc.\n",
        "\n",
        "Let's test the network on our input image tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vV2yyRBtanG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71e881b-33d8-41f7-e465-aaf0f02f3486"
      },
      "source": [
        "convnet(img)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0718, -0.0638,  0.0020,  0.0630,  0.0235, -0.0152, -0.0879, -0.0201,\n",
              "          0.0778,  0.1134]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euV9k1o7te-Z"
      },
      "source": [
        "The network produces a 10-dimensional vector according to plan. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g1oIKHruKqm"
      },
      "source": [
        "And now we can define our training and testing code for each epoch. This is litteraly copy-pasted from the code we used last week. These are the benefits of a well-designed deep learning framework like pytorch: the code for doing things like training and testing needs to change very little despite the architectures changing significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPbVO3A2t4kc"
      },
      "source": [
        "def train(net,dataloader, optimizer,loss_fun):\n",
        "  net.train()\n",
        "  for x, t in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    L = loss_fun(net(x), t)\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def test(net,dataloader,loss_fun):\n",
        "  net.eval()\n",
        "  total_L = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for x, t in dataloader:\n",
        "      out = net(x)\n",
        "      total_L += loss_fun(out, t)\n",
        "      _,pred = out.max(dim=1)     # this counts how many we got right\n",
        "      correct += (pred==t).sum() \n",
        "  total_L /= len(dataloader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    total_L, correct, len(dataloader.dataset),\n",
        "    100. * correct / len(dataloader.dataset)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11nFogE3u3Cu"
      },
      "source": [
        "Unfortunately the only problem here is that our code is CPU based. Because convolutions are so computationally demanding, this will take quite a long time to train. Try it out by running just a single epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVjyhdVBvGRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01de873-3e63-415b-e2a8-11d372e33dc0"
      },
      "source": [
        "from time import perf_counter\n",
        "\n",
        "tic = perf_counter()\n",
        "\n",
        "optim_SGD = optim.SGD(convnet.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "train(convnet, train_loader, optim_SGD, nn.CrossEntropyLoss())\n",
        "test(convnet, test_loader,  nn.CrossEntropyLoss(reduction='sum'))\n",
        "\n",
        "toc = perf_counter()\n",
        "print(f\"One epoch of training took {toc - tic:0.4f} seconds\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.2976, Accuracy: 1051/10000 (11%)\n",
            "\n",
            "One epoch of training took 37.3649 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW_FzGcavJSJ"
      },
      "source": [
        "It seems one epoch takes about a minute. Deep learning researchers are notoriously impatient when training neural nets. We really don't like to wait, life is just too short! Which is why GPU computing comes to the rescue. Hopefully you are running this code on a GPU-enabled computer (e.g. google colab) which means that with a couple of lines of code you can reap the benefits of super-fast training of deep convnets. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gK_HywAYGtU"
      },
      "source": [
        "## Accelerating code with GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08UZBoNIYEiZ"
      },
      "source": [
        "We can exploit GPU programming in a variety of ways. The simplest way is to use the `.cuda()` method on a layer, network or a tensor. E.g."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_87bQcVOwMsD"
      },
      "source": [
        "#define tensor\n",
        "A = torch.rand(1,3,32,32)\n",
        "\n",
        "# define a conv layer\n",
        "conv = nn.Conv2d(3, 16, 5)\n",
        "\n",
        "# move both tensor and layer to the GPU memory\n",
        "A = A.cuda()\n",
        "conv = conv.cuda()\n",
        "\n",
        "# perform convolution on GPU\n",
        "B = conv(A)\n",
        "\n",
        "# bring result of computation back to CPU memmory\n",
        "B = B.cpu()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWRARn-Yxpfo"
      },
      "source": [
        "A slightly more flexible way of achieving the same result would be to use the `.to()` method, which allows us to specify programmaticaly which device we'd like to carry out the computation on. That way if our computer does not have a cuda-enabled GPU we can fall back to slower CPU computations. We can always find out if our computer supports fast GPU computations by using the `torch.cuda.is_available()` command. We can neatly create a torch device object that is either a GPU card or the plain old CPU, depending on our system capabilities, with the following snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th7G5lorxo4Z"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVWR9QYxy71N"
      },
      "source": [
        "We can then achieve the same result as the code above with code that will work in non-cuda enabled systems:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYXsOphfy01y"
      },
      "source": [
        "#define tensor\n",
        "A = torch.rand(1,3,32,32)\n",
        "\n",
        "# define a conv layer\n",
        "conv = nn.Conv2d(3, 16, 5)\n",
        "\n",
        "# move both tensor and layer to the GPU memory\n",
        "A = A.to(device)\n",
        "conv = conv.to(device)\n",
        "\n",
        "# perform convolution on GPU\n",
        "B = conv(A)\n",
        "\n",
        "# bring result of computation back to CPU memory (if it is already in the CPU this will not do any extra copy)\n",
        "B = B.cpu()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT1oYpxCziYD"
      },
      "source": [
        "Pytorch allows us to create tensors directly to the device that we want, instead of in CPU memory first and then copying across to the GPU. So our `A` tensor above can be constructed as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRahTj31z9l-"
      },
      "source": [
        "#define tensor\n",
        "A = torch.rand(1,3,32,32, device = device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qvdPz8R14aH"
      },
      "source": [
        "We can now rewrite our network creation line as: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsMCw9Gn13ph"
      },
      "source": [
        "convnet = ConvNet().to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpFBKYCe2Ixk"
      },
      "source": [
        "And the training and testing code must move the data across to the GPU if available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LydAUpLgit6B"
      },
      "source": [
        "def train_gpu(net,dataloader, optimizer,loss_fun):\n",
        "  net.train()\n",
        "  for x, t in dataloader:\n",
        "    x = x.to(device)\n",
        "    t = t.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    L = loss_fun(net(x), t)\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def test_gpu(net,dataloader,loss_fun):\n",
        "  net.eval()\n",
        "  total_L = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for x, t in dataloader:\n",
        "      x = x.to(device)\n",
        "      t = t.to(device)\n",
        "      out = net(x)\n",
        "      total_L += loss_fun(out, t)\n",
        "      _,pred = out.max(dim=1)     # this counts how many we got right\n",
        "      correct += (pred==t).sum() \n",
        "  total_L /= len(dataloader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    total_L, correct, len(dataloader.dataset),\n",
        "    100. * correct / len(dataloader.dataset)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4WNcBN2inG"
      },
      "source": [
        "The training code now runs much faster. Let's try one epoch now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPzCW98iArPm",
        "outputId": "2407d344-5ad4-4d51-bb6b-aee16c27a4af"
      },
      "source": [
        "from time import perf_counter\n",
        "\n",
        "tic = perf_counter()\n",
        "\n",
        "optim_SGD = optim.SGD(convnet.parameters(), lr=0.01, momentum=0.5)\n",
        "train_gpu(convnet, train_loader, optim_SGD, nn.CrossEntropyLoss())\n",
        "test_gpu(convnet, test_loader,  nn.CrossEntropyLoss(reduction='sum'))\n",
        "\n",
        "toc = perf_counter()\n",
        "print(f\"One epoch of gpu-enabled training took {toc - tic:0.4f} seconds\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.2942, Accuracy: 1157/10000 (12%)\n",
            "\n",
            "One epoch of gpu-enabled training took 6.6909 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asxljPQE6ARH"
      },
      "source": [
        "We can now define the epoch code more generally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlKIANtZ6DjL"
      },
      "source": [
        "def run_epochs(num_of_epochs, net, train_loader, test_loader):\n",
        "    optim_SGD = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)    \n",
        "    for e in range(num_of_epochs):\n",
        "        print(f\"Epoch: {e+1}/{num_of_epochs}. Training ...\")\n",
        "        train_gpu(net, train_loader, optim_SGD, nn.CrossEntropyLoss())\n",
        "        print(\"Testing ...\")\n",
        "        test_gpu(net, test_loader,  nn.CrossEntropyLoss(reduction='sum'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA4ptMJHivDd"
      },
      "source": [
        "convnet = ConvNet().to(device)\n",
        "run_epochs(100, convnet, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSpGcviIYhfM"
      },
      "source": [
        "## Going deep: improving performance by deeper architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAgfxk0WNtHi"
      },
      "source": [
        "We have seen how a basic convnet is already outperforming fully connected layers by leveraging translation invariance. Convnets can perform the same operations on images with far less parameters, which makes them easier to train. But can we do even better? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py5Qivm63bzE"
      },
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.flatten(1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9WpAe_V-uf4"
      },
      "source": [
        "vggnet = VGG('VGG16').cuda()\n",
        "#run_epochs(100, vggnet, train_loader, test_loader)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_o4R4BdmsPE",
        "outputId": "63600e9c-c173-45ac-f353-ce7bbdc1eaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "85155b43cac64c4ba438e4d9bf48a71f",
            "583a90f0284b4241b0f8b54001372b68",
            "3552085cf9c24391ab4e629808af11b3",
            "5b479cb13a774746a69bb8d12112f839",
            "e6a9dc68c57c49a3a2cf66593b322d97",
            "5de1a21dfe114398991d765ab990c45d",
            "56e91bdf03174f8894c9657c1e8c7529",
            "beb21987ed1c487e896e88108234fd88"
          ]
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85155b43cac64c4ba438e4d9bf48a71f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-9Zcz6hmtSi"
      },
      "source": [
        "vggnet = VGG('VGG16').cuda()\n",
        "run_epochs(100, vggnet, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb_YRFXPnLYu"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdEnq-AbI0l0"
      },
      "source": [
        "writer = SummaryWriter()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLyu5NrSI25t"
      },
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "grid = torchvision.utils.make_grid(images)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so2Wn_DXKkuw"
      },
      "source": [
        "vggnet = vggnet.cpu()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEbvRrt0I8I0"
      },
      "source": [
        "writer.add_image('images', grid, 0)\n",
        "writer.add_graph(vggnet, images)\n",
        "writer.close()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqnPovf8SIoJ"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "\n",
        "for n_iter in range(100):\n",
        "    writer.add_scalar('Loss2/train', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Loss2/test', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Accuracy2/train', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Accuracy2/test', np.random.random(), n_iter)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjmlDawxKS_F"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woQYvbSkJBfz"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mHigDGQKKa5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for n_iter in range(100):\n",
        "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
        "\n",
        "writer.flush()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ut9cYGLX8F"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "writer.add_graph(convnet.cpu(), images)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGPwXMMTS3Nm"
      },
      "source": [
        "writer.flush()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejYQJDluS4sV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}