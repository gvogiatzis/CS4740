{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_05.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2xXzaM5YZGbSPjZ/tgAEI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLVrEeFtaICL"
      },
      "source": [
        "# [CS4740 Labs] Week 5: Exploring the capabilities of Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5mppE4be1zo"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4MZ5yQgaUSs"
      },
      "source": [
        "In this lab we will carry out a few experiments with Recurrent Neural Networks in order to understand the way these models operate and also to see some interesting applications. RNNs are Deep Learning models that are particularly suitable for sequential data such as time series, text, speech, audio signals etc.\n",
        "\n",
        "We can think of an RNN as a *machine* that is fed a sequence of data-points. After each data-point is entered, the machine performs some calculations based on the information received, and it  modifies its inner state appropriately. It then produces some output before moving to the next data-point in the sequence. \n",
        "\n",
        "We can use RNNs in a multitude of ways:\n",
        "\n",
        "* We can use the last RNN output after a sequence has been read, to make some inference about the type of sequence. E.g. classifying a bit of text into one of a number of categories.\n",
        "* We can use an RNN to encode an input sequence, and pass a code to a second RNN that decodes that into an output sequence. This can form the basis of a sequence-to-sequence mapping, e.g. translation of english to french text.\n",
        "* We can use an RNN as a next-step predictor. E.g. predicting the next day's stock market price from its past history.\n",
        "\n",
        "In this lab we will see a few examples of RNNs: \n",
        "\n",
        "1. A very simple bit-counter\n",
        "2. A text generator\n",
        "3. An *adder* that parses numerical expressions\n",
        "\n",
        "Firstly let's import a few packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsD64X1cnGv5"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "from textblob import Word\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from more_itertools import sliced\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from random import randint\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUT9QqwnrjmM"
      },
      "source": [
        "## A simple running total calculator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoVr2i_OsB_T"
      },
      "source": [
        "This is perhaps the simplest possible RNN we can come up with, that still has a usefull function. The idea is simple: we feed a sequence of numbers and the RNN calculates a running total over those numbers as they come in. \n",
        "\n",
        "First, let's see how the rnn layers work in practice. The main two choices are the GRU and the LSTM, details of which we have seen in lectures. We will be using the GRU throughout this lab but both are drop-in replacements for each other.\n",
        "\n",
        "Let's define a simple GRU layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sd7HbgBskbE"
      },
      "source": [
        "gru = nn.GRU(input_size=2, hidden_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdJo2k-JxPD2"
      },
      "source": [
        "This module expects to receive sequences of 2-dimensional data points. It crunches them using a hidden layer of size 5, which means that it outputs a sequence of 5-element vectors. Now the actual dimensionality of the input and output is somewhat confusing because of the complexity of batches: it is very important that all NN layers in pytorch are able to handle batched input for standard stochastic gradient descent to work. \n",
        "\n",
        "So the input to RNN layers (by default) is of size \n",
        "\n",
        " `sequence_size` x `batch_size` x `input_size`\n",
        "\n",
        "Why is `batch_size` in the middle I hear you ask? Well it makes sense if we want to iterate through the elements of the sequence. Each element in that sequence is of size `batch_size` x `input_size` and that's exactly what the inner layers of the RNN expect when operating on batches!\n",
        "\n",
        "Having said that, if it is more natural for you to think in terms of batches as the rightmost index, as you would do for a conv or linear layer, `pytorch` allows you to do that by using the `batch_first=True` flag inside RNN constructors. In fact we will be making use of that flag throughout this lab, but bear in mind that `pytorch` is flipping the indices internally.\n",
        "\n",
        "So in our case we would define an input consisting of a batch of 64 sequences, each of which has 100 elements, that are 2-dimensional, we would do it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJzCvbFPssEM"
      },
      "source": [
        "gru = nn.GRU(input_size=2, hidden_size=5)\n",
        "\n",
        "x = torch.randn(100,64,2)\n",
        "y,h = gru(x)\n",
        "\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1lIbEuWz3nx"
      },
      "source": [
        "But if we use the `batch_first=True` flag, this becomes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdGFoYqFz85B"
      },
      "source": [
        "gru = nn.GRU(input_size=2, hidden_size=5, batch_first=True)\n",
        "\n",
        "x = torch.randn(64,100,2)\n",
        "y,h = gru(x)\n",
        "\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqZtGMq0LoV"
      },
      "source": [
        "Now back to our running total network, the input will be one-dimensional while we can use a relatively small hidden layer of size 64. This is because the task is quite simple so should be easy to handle with this small number of nodes. We will however need to map those 64 nodes to a 1-dimensional output which is the running total. The best way to do that is with a linear layer from 64 to 1 dimension. Alltogether this looks like this:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siTTVNNMnIzy"
      },
      "source": [
        "class RunningTotalNet(nn.Module):\n",
        "    def __init__(self, hidden_dim=64):\n",
        "        super(RunningTotalNet, self).__init__()\n",
        "        self.rnn = nn.GRU(input_size=1,\n",
        "                        hidden_size=hidden_dim,\n",
        "                        batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, batch_size=1):\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC10RVou0_j6"
      },
      "source": [
        "Now we can write some training code. We can generate the data randomly for each batch we train on, using the torch `randint` command that will generate for us random integers from 0 to 9. Each batch will be of size \n",
        "\n",
        "`(batch_size,sequence_length,1)`\n",
        "\n",
        "We can also easily obtain the running total output that we want our network to learn, using the `cumsum` command that computes the cummulative summation over a particular tensor dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugpeiR3hpdNT"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size=100\n",
        "sequence_length=50\n",
        "num_of_epochs = 5000\n",
        "net = RunningTotalNet().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "loss = nn.MSELoss()\n",
        "for e in range(num_of_steps):\n",
        "    x = torch.randint(high=10, (batch_size,sequence_length,1), device=device, dtype=torch.float32)\n",
        "    t = x.cumsum(dim=1,dtype=torch.float32)\n",
        "    y = net(x)\n",
        "    L = loss(y,t)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"\\rEpoch: {e}/{num_of_epochs} \\tL={L}\", end=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPHsKve4WdF"
      },
      "source": [
        "Let's evaluate how well the network has learned:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4_L_WfWm7nU"
      },
      "source": [
        "def evaluate_net(net, input):\n",
        "    x = torch.tensor(input, device=device, dtype=torch.float32).view(1,-1,1)\n",
        "    y = net(x)\n",
        "    return y.squeeze().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P700RcNz4due"
      },
      "source": [
        "Trying out a few different sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoS-Habjn-AB"
      },
      "source": [
        "print(evaluate_net(net,[1,0,0,5]))\n",
        "print(evaluate_net(net,[1,2,3,4]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqRlJF4F4oHy"
      },
      "source": [
        "What about negative numbers or bigger than 9?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34cNBPlP4nRZ"
      },
      "source": [
        "print(evaluate_net(net,[-1,-1,0,-2]))\n",
        "print(evaluate_net(net,[0,20,0,15]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HXU2Svv48m9"
      },
      "source": [
        "The network shows signs that it has trully learned to accumulate numbers, beyond just memorizing its input! \n",
        "\n",
        "Here's some more detailed code that plots side by side the expected output vs what the network produces, for a randomly picked sequence of numbers. Play with the `low` and `high` parameters to stress the network outside its comfort zone!\n",
        "\n",
        "> For the curious student:  can you check what happens if we increase the size of the hidden layer? Will the network overfit and stop generalizing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcNetLEz2rqP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = torch.randint(low=-2, high=20, size=(1,15,1), device=device, dtype=torch.float32)\n",
        "t = x.cumsum(dim=1,dtype=torch.float32)\n",
        "t = t.squeeze().tolist()\n",
        "y = net(x).squeeze().tolist()\n",
        "\n",
        "plt.plot(t,'r-')\n",
        "plt.plot(y, 'b+')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Q3ioUUOyfo"
      },
      "source": [
        "## A random text generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yteaKFPgOyfs"
      },
      "source": [
        "In this section we will use a character-level RNN as a generator of text. The idea is to create a NN that can predict the next character from a string of all the previous ones. Then we can feed the predicted character back to the network so that it can predict the next one, and the next after that etc.\n",
        "\n",
        "First, we need some more libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR30AbSPOyfs"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from more_itertools import sliced\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-1m__CrOyft"
      },
      "source": [
        "We will use a well known publicly available dataset consisting of news articles from BBC news. The articles each fall under one of five classes, and can be used for document classification. Here we will only be using the raw text of all the news stories, combined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JI0hNusOyft"
      },
      "source": [
        "! wget https://github.com/suraj-deshmukh/BBC-Dataset-News-Classification/raw/master/dataset/dataset.csv -O dataset.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDfaSAwDOyfu"
      },
      "source": [
        "The following snippet opens the csv file and then from every row, takes the \"news\" field that contans the news story as a string. All these news stories are then concatenated using the `join` command. We will only be using the first million characters, just to keep things manageable in the space of a single lab session. Feel free to experiment with using the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEARjULMOyfu"
      },
      "source": [
        "with open('dataset.csv', newline='', encoding = \"ISO-8859-1\") as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    all_text = \"\".join(row['news'] for row in reader)\n",
        "\n",
        "all_text = all_text[:1000000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07upJG6NOyfu"
      },
      "source": [
        "As is commonly the case in Deep Learning methods for natural language processing, we must first turn individual characters (or words if we have a word-level RNN) into indices (i.e. integer numbers). The best way to do this is using a python dictionary. In fact we will define python dictionaries that map an index to a character (`itoc`) as well as a character to its index (`ctoi`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPnoK9d0Oyfv"
      },
      "source": [
        "all_chars = sorted(set(all_text))\n",
        "num_of_characters = len(all_chars)\n",
        "itoc = {i:c for i,c in enumerate(all_chars)} # this is just for clarity, this dict is same as all_chars \n",
        "ctoi = {c:i for i,c in enumerate(all_chars)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQILUpeSOyfv"
      },
      "source": [
        "Now we can easily convert a sequence of characters into a sequence of numbers (indices). However as is usually the case with categorical variables (i.e. those that can take one out of a finite number of possible values) they must be converted into vectors (e.g. one-hot encodings) before we can process them further. The reasons are somewhat complex, but fundamentally it's because integer indices have a natural ordering. E.g. index 2 is somehow 'close' to index 3. If we would like our algorithm to take that into account then integer encodings are fine. Usually however we don't want to assume any ordering and in these cases we are obliged to use vector representations. \n",
        "\n",
        "In fact we will go one step further than simple one-hot encodings. We will define a general, learnable vector for each of the characters. This is easily achieved using what is known as an embedding layer.\n",
        "\n",
        "Consider just for illustration's sake a very simple 4-dimensional embedding that maps from a character index to a 4D vector. This is defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0kvfTPDOyfv"
      },
      "source": [
        "embedding = nn.Embedding(num_of_characters, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMiIP20hOyfv"
      },
      "source": [
        "This is a structure that produces a different 4-d vector for each of the different characters in our character set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAb_0dsVOyfw"
      },
      "source": [
        "embedding(torch.tensor(ctoi['s']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33wCCIhIWwl6"
      },
      "source": [
        "Let's apply it to a longer string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDfUmGzfW2We"
      },
      "source": [
        "# a string variable\n",
        "txt = 'hello' \n",
        "\n",
        "# converting it into an array of indices\n",
        "x = [ctoi[c] for c in txt]\n",
        "\n",
        "# converting into a tensor\n",
        "x = torch.tensor(x)\n",
        "\n",
        "y = embedding(x)\n",
        "\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkLrAEAqOyfw"
      },
      "source": [
        "Getting the dimensions right in `pytorch` is always tricky. That's why it's important to try out the layers with toy input data, to make sure it all fits together as expected. \n",
        "\n",
        "We are now ready to define a network that will predict the next character given some text. This is essentially a classification task. Because we want our network to be able to use input sequences of variable length we will use a recurrent network in the middle of the computation. Here we chose a GRU, but an LSTM will also work.\n",
        "\n",
        "The structure of the predictor network is as follows:\n",
        "\n",
        "input char sequence -> embedding -> GRU -> fully connected layer -> softmax\n",
        "\n",
        "As we will be using a `CrossEntropyLoss` loss function, we can omit the softmax layer in the end. The code looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1tdwdbbOyfw"
      },
      "source": [
        "class CharPredictorNet(nn.Module):\n",
        "    def __init__(self, charset_size, embed_size=100, hidden_dim=1024):\n",
        "        super(CharPredictorNet, self).__init__()\n",
        "        self.embedding = nn.Embedding(charset_size, embed_size)\n",
        "        self.lstm = nn.GRU(input_size=embed_size,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, charset_size)\n",
        "        self.h=None\n",
        "\n",
        "    def forward(self, x, keep_memory=False):\n",
        "        x = self.embedding(x)\n",
        "        if keep_memory: # if keep_memory, we use the memory vector from previous run\n",
        "            x, self.h = self.lstm(x,self.h) \n",
        "        else:           # else we start a new run (memory is initialized to zero)\n",
        "            x, self.h = self.lstm(x) \n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0BXnXvzYsfY"
      },
      "source": [
        "Testing that it all works in terms of the dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-KvKTgtV1Cj"
      },
      "source": [
        "x = torch.tensor([1,2,0,2,50,20])\n",
        "net = NextCharPredictor(charset_size=100)\n",
        "y = net(x.view(1,-1))\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D23_RNswOyfw"
      },
      "source": [
        "Remeber that RNNs in pytorch can handle mini-batches. If you use the `batch_first=True` flag, then the networ expects inputs of shape:\n",
        "\n",
        "batch_size x sequence_size x element_size\n",
        "\n",
        "\n",
        "where batch_size is the number of sequences contained in your batch, sequence_size is the length of each sequence and element_size is the size of the vector that represents each sequence element (in our case a character embedding vector)\n",
        "\n",
        "It will come handy to define a simple running average class to keep track of the average accuracy accross an epoch. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxWw2M8ROyfw"
      },
      "source": [
        "class RunningAverage:\n",
        "    def __init__(self):\n",
        "        self.n=0\n",
        "        self.tot=0\n",
        "    \n",
        "    def add(self,x):\n",
        "        self.n += 1\n",
        "        self.tot += x\n",
        "        \n",
        "    def __call__(self):\n",
        "        return self.tot/self.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdpcySDVOyfx"
      },
      "source": [
        "As before, we will start up a tensorbord GUI to monitor our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKKznjmKOyfx"
      },
      "source": [
        "tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GuAIQdkVNFH"
      },
      "source": [
        "We are now ready to write the training code. In each epoch we will be using a chunk of the whole text that is stored in the `all_text` variable. This chunking is done by the `sliced` command that can be found in the `more_itertools` package. Each chunk will have length \n",
        "\n",
        "`batch_size*seq_length+1`\n",
        "\n",
        "and will be used to form `batch_size` sequences, each of length `seq_length`. We can use the same string to generate both inputs and expected outputs by exploiting the fact that the output is just the same as the input but shifted by one position to the right. I.e. the network has to predict the next character, having seens all characters up to, but not including that charracter. \n",
        "\n",
        "We use a neat little python idiom that does the shifting. If variable `txt` holds string `'hello world'` then the two variables `txt[:-1]` and `txt[1:]` hold \n",
        "\n",
        "`'hello worl'`\n",
        "\n",
        "and\n",
        "\n",
        "`'ello world'`\n",
        "\n",
        "respectively. Which means that we can use `txt[:-1]` as the input and `txt[1:]` as the target. If our chunk `txt` has length \n",
        "\n",
        "`batch_size*seq_length+1`\n",
        "\n",
        "then `txt[:-1]` and `txt[1:]` each have length \n",
        "\n",
        "`batch_size*seq_length`\n",
        "\n",
        "so we will be able to reshape them into a `batch_size` x `seq_length` tensor for further processing inside the network.\n",
        "\n",
        "The training code looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZLMBFbVOyfx"
      },
      "source": [
        "num_of_epochs = 50\n",
        "seq_length = 100\n",
        "batch_size=64*2\n",
        "embed_size=100\n",
        "hidden_dim=1024\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = CharPredictorNet(embed_size=embed_size, hidden_dim=hidden_dim, charset_size = num_of_characters).to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(net.parameters(), lr=0.001) \n",
        "\n",
        "max_iter = int(len(all_text)/(batch_size*seq_length+1))\n",
        "\n",
        "writer = SummaryWriter(f'runs/exp_1M_rnd_batch128')\n",
        "\n",
        "for e in range(num_of_epochs):\n",
        "    train_acc = RunningAverage()\n",
        "    for i,txt in enumerate(sliced(all_text, batch_size*seq_length+1)):\n",
        "        if len(txt)<batch_size*seq_length+1:\n",
        "            break\n",
        "        x = torch.tensor([ctoi[c] for c in txt[:-1]], device = device)\n",
        "        t = torch.tensor([ctoi[c] for c in  txt[1:]], device = device)        \n",
        "        y = net(x.view(batch_size,-1)) # we must wrap so it can be processed as a batch\n",
        "        y = y.view(-1,num_of_characters) # we unwrap y to compare it with the unwrapped target\n",
        "        L = loss(y, t)\n",
        "        acc = sum(y.argmax(dim=1)==t).item()/(batch_size*seq_length)\n",
        "        train_acc.add(acc)\n",
        "        print(f\"\\rEpoch: {e}/{num_of_epochs} Iter: {i}/{max_iter}\\tacc={100*acc:0.2f}%\\tL={L}\", end=\"\")\n",
        "        optim.zero_grad()\n",
        "        L.backward()     \n",
        "        optim.step()\n",
        "    writer.add_scalar('Accuracy', train_acc(), e)\n",
        "    writer.flush()\n",
        "    print(f\"\\rEpoch: {e}/{num_of_epochs} Average acc: {train_acc()}\")\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fngF1NS8bRMM"
      },
      "source": [
        "The following is a code snippet that runs the network in a generative way. Taking its output and feeding it back in as input we can keep on generating text for arbitrary lengths. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vBQDGoqOyfx"
      },
      "source": [
        "def generate_text(net, seed_txt, length=100):\n",
        "    indices = [ctoi[c] for c in seed_txt]\n",
        "    x = torch.tensor(indices, device = device)\n",
        "    \n",
        "    x = net(x) # compute network predicted probabilities for next char\n",
        "    x = x.argmax(dim=1).view(-1) # get most probable char according to network\n",
        "    x = x[-1] # take the last character in the output to use as input\n",
        "    indices.append(x.item()) #add to text \n",
        "    for i in range(length):\n",
        "        x=net(x, keep_memory=True) # compute network predicted probabilities for next char\n",
        "        x = x.argmax(dim=1).view(-1)# get most probable char according to network\n",
        "        indices.append(x.item()) #add to text\n",
        "    return  \"\".join(itoc[i] for i in indices) # convert back to string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUj6xThrf8M0"
      },
      "source": [
        "Try out a few seed texts to see how the network completes them "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TgfF_AHOyfy"
      },
      "source": [
        "print(generate_text(net, \"chine\", length=200))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw-QEWUHOdoQ"
      },
      "source": [
        "\n",
        "\n",
        "> For the curious: Can you try the algorithm on a different dataset? E.g. you might want to use some of the programming language code you have written for other modules. Try to see if the network learns how to write passable python/java programs! \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUhU8tQ96FIe"
      },
      "source": [
        "## An interpreter of numerical expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jod4TTpeMllz"
      },
      "source": [
        "We will now look at a slightly more challenging problem, that of parsing and interpreting numerical expresions. This may sound like a simple problem but some consideration will convince you it is non-trivial to solve using a general-purpose computational device such as a neural network. \n",
        "\n",
        "As an example, our network will be fed a sequence such as \n",
        "\n",
        "`'125+238='` \n",
        "\n",
        "and it will be able to respond with the correct answer which is\n",
        "\n",
        "`'363.'`\n",
        "\n",
        "For the purposes of this lab we will restrict the problem to just two integers between 0 and 999 and they will only be combined using additions. Firstly, we must be able to automatically generate training data. This is done with the following two functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBkYRRwGMPF6"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from more_itertools import sliced\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "itoc=list('0123456789+=. ')\n",
        "ctoi = {c:i for i,c in enumerate(itoc)}\n",
        "\n",
        "def create_strings(a,b):\n",
        "    c=a+b\n",
        "    a_s=str(a)\n",
        "    b_s=str(b)\n",
        "    c_s=str(c)\n",
        "    input = f'{a_s}+{b_s}={c_s}'\n",
        "    output = f'{c_s}.'\n",
        "    input = f'{a_s}+{b_s}'\n",
        "    output=' ' * len(input)\n",
        "    input += f'={c_s}'\n",
        "    output+= f'{c_s}.'\n",
        "\n",
        "    \n",
        "    return input, output\n",
        "\n",
        "def create_addition_samples(num_samples,ndigits):\n",
        "    x,t=[],[]\n",
        "    for n in range(num_samples):\n",
        "        a = randint(0,10**ndigits-1)\n",
        "        b = randint(0,10**ndigits-1)\n",
        "        input, output = create_strings(a,b)\n",
        "        input = input.ljust(3*ndigits+3, ' ')\n",
        "        output = output.ljust(3*ndigits+3, ' ')\n",
        "        x.append([ctoi[c] for c in input])\n",
        "        t.append([ctoi[c] for c in output])\n",
        "    return x,t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AggT8mgJsY11"
      },
      "source": [
        "Take a while to study how these functions work. Also generate a few strings for some numbers. E.g."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7r8NZrQrwZ1"
      },
      "source": [
        "x,y = create_strings(150,82)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqZpDUNsgOA"
      },
      "source": [
        "The idea works very much like the next-character predictor that we created above. In fact the neural network architecture is identical, we don't need a new class. \n",
        "\n",
        "The input sequence is the arithmetic expression and the output is a series of space characters, until the network encounters the `'='` character. At that point the network is required to start outputing the result of the addition. This is very similar to the offsetting of input and output by one character that we saw before. The end effect is that, if the network manages to learn this, as soon as it encounters `'='`, it will perform the calculation and output the result. \n",
        "\n",
        "The same effect could probably be achieved with a seq2seq architecture with an encoder and a decoder RNN. That would perhaps be a more general solution. However this problem is simple enough that a single network can play both roles.\n",
        "\n",
        "The training code is very straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "749bTBqANrty"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size=100\n",
        "num_of_epochs = 2000\n",
        "ndigits=3\n",
        "net = CharPredictorNet(charset_size = len(itoc),embed_size=32, hidden_dim=2048).to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "for e in range(num_of_epochs):\n",
        "    x,t = create_addition_samples(batch_size,ndigits=ndigits)\n",
        "    x = torch.tensor(x,device=device)\n",
        "    t = torch.tensor(t,device=device)\n",
        "    y = net(x)\n",
        "    L = loss(y.view(-1,len(itoc)),t.view(-1))\n",
        "    optimizer.zero_grad()\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"\\rEpoch: {e}/{num_of_epochs} \\tL={L}\", end=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT686UmYuD63"
      },
      "source": [
        "We now write a simple function, similar to the one for the text generator, to evaluate the numerical expression interpreter network. Instead of priming the RNN with a bit of text we would like to continue, we prime it with the mathematical expression we would like to interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYezzKizts9-"
      },
      "source": [
        "def eval_adder(net, input_str):\n",
        "    itoc=list('0123456789+=. ')\n",
        "    ctoi = {c:i for i,c in enumerate(itoc)}\n",
        "    input = torch.tensor([[ctoi[c] for c in input_str]], device = device)\n",
        "    x = net(input).argmax(dim=2)\n",
        "    x = x[0,-1].view(1,-1)\n",
        "    output = [x.item()]\n",
        "    total_len=0\n",
        "    while x.item() != ctoi['.'] and total_len<10:\n",
        "        x = net(x, keep_memory=True).argmax(dim=2)\n",
        "        output.append(x.item())\n",
        "        total_len+=1\n",
        "    return \"\".join(itoc[i] for i in output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3muu6OKukqY"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LDnmYwzytWd"
      },
      "source": [
        "eval_adder(net, \"555+234=\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlEyjh0Cp_mO"
      },
      "source": [
        "eval_adder(net, \"999+999=\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoTXcZ0-unGq"
      },
      "source": [
        "It seems to work! \n",
        "\n",
        "\n",
        "\n",
        "> For those of you who would like to investigate further:\n",
        "\n",
        "* Can you find some cases where the interpreter fails? \n",
        "* What about examples that have not been shown to the network?\n",
        "* Can you extend this idea to a fully fledged interpreter that covers `+,-,*,/`? Maybe even parentheses? Have fun!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvgzJ2A4qCUX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}