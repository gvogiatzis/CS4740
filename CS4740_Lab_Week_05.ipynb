{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfm/VZaSxmVxTN8MHgyOBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh8vj7alPWAM"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from textblob import Word\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7CLRcOslKk-",
        "outputId": "5677d80d-d2dc-4780-acdc-d03272118cd0"
      },
      "source": [
        "! wget https://github.com/suraj-deshmukh/BBC-Dataset-News-Classification/raw/master/dataset/dataset.csv -O dataset.csv\n",
        "! python -m textblob.download_corpora"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-21 15:34:58--  https://github.com/suraj-deshmukh/BBC-Dataset-News-Classification/raw/master/dataset/dataset.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/suraj-deshmukh/BBC-Dataset-News-Classification/master/dataset/dataset.csv [following]\n",
            "--2021-01-21 15:34:59--  https://raw.githubusercontent.com/suraj-deshmukh/BBC-Dataset-News-Classification/master/dataset/dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5108610 (4.9M) [text/plain]\n",
            "Saving to: ‘dataset.csv’\n",
            "\n",
            "dataset.csv         100%[===================>]   4.87M  21.9MB/s    in 0.2s    \n",
            "\n",
            "2021-01-21 15:35:00 (21.9 MB/s) - ‘dataset.csv’ saved [5108610/5108610]\n",
            "\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zrROmdJQIKW"
      },
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for datasets.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"\\'s\", \"\", string)\n",
        "    string = re.sub(r\"\\'ve\", \"\", string)\n",
        "    string = re.sub(r\"n\\'t\", \"\", string)\n",
        "    string = re.sub(r\"\\'re\", \"\", string)\n",
        "    string = re.sub(r\"\\'d\", \"\", string)\n",
        "    string = re.sub(r\"\\'ll\", \"\", string)\n",
        "    string = re.sub(r\",\", \"\", string)\n",
        "    string = re.sub(r\"!\", \"\", string)\n",
        "    string = re.sub(r\"\\(\", \"\", string)\n",
        "    string = re.sub(r\"\\)\", \"\", string)\n",
        "    string = re.sub(r\"\\?\", \"\", string)\n",
        "    string = re.sub(r\"'\", \"\", string)\n",
        "    string = re.sub(r\"`\", \"\", string)\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"[0-9]\\w+|[0-9]\",\"\", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "data = pd.read_csv('dataset.csv', encoding = \"ISO-8859-1\")\n",
        "doctxt = data['news'].tolist()\n",
        "doctype = data['type'].tolist()\n",
        "\n",
        "docs= [[Word(w).lemmatize() for w in clean_str(s).split()] for s in doctxt]\n",
        "index_to_term = sorted(list(set().union(*docs)))\n",
        "vocab_size = len(index_to_term)\n",
        "term_to_index = {t:i for i,t in enumerate(index_to_term)}\n",
        "\n",
        "data = [[ term_to_index[w] for w in doc] for doc in docs]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvOKH_gjYFyh"
      },
      "source": [
        "vocab_size = len(index_to_term)\n",
        "embed_size = 300\n",
        "hidden_dim=128\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)\n",
        "lstm = nn.LSTM(input_size=embed_size,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srkjqx5gqmzk",
        "outputId": "4ce6c332-88f7-4fcb-bbaa-4479dc246b91"
      },
      "source": [
        "len(data[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pbIFG4oqa7f",
        "outputId": "3e3acb0d-121b-491c-901f-0d2ae4899e5f"
      },
      "source": [
        "x = torch.tensor(data[0])\n",
        "x = embedding(x.unsqueeze(0))\n",
        "x.shape\n",
        "y,h = lstm(x)\n",
        "y.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 445, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4bWr4nPt6ju",
        "outputId": "44305d54-43cb-49fb-fac3-0127dba3c435"
      },
      "source": [
        "h[1].shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJIxueg5loOY"
      },
      "source": [
        "class TextClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size=300, hidden_dim=128):\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_size=embed_size,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        self.lstm(x)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
        "        out_reverse = output[:, 0, self.dimension:]\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "        text_fea = self.drop(out_reduced)\n",
        "\n",
        "        text_fea = self.fc(text_fea)\n",
        "        text_fea = torch.squeeze(text_fea, 1)\n",
        "        text_out = torch.sigmoid(text_fea)\n",
        "\n",
        "        return text_out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}