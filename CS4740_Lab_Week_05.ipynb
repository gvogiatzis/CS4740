{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_05.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPakzsBTu9sZNrlhDR3pojx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrFBLDqnCbC6"
      },
      "source": [
        "# Generating text with Recurrent Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0H3Gfw1CkNn"
      },
      "source": [
        "In this lab we will use a character-level RNN as a generator of text. In the process we will find out about the key API elements in `pytorch` that concern RNNs. First of all, we need to import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fedrzcSJ_K4E"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "from textblob import Word\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from more_itertools import sliced\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jf0KYo1GIx8"
      },
      "source": [
        "For the purposes of this lab we will be using a well known publicly available dataset consisting of news articles from BBC news. The articles each fall under one of five classes, and can be used for document classification. Here we will only be using the raw text of all the news stories, combined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZcXwSx_Xh1"
      },
      "source": [
        "! wget https://github.com/suraj-deshmukh/BBC-Dataset-News-Classification/raw/master/dataset/dataset.csv -O dataset.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3svzwBL8HnJ5"
      },
      "source": [
        "The following snippet opens the csv file and then from every row, takes the \"news\" field that contans the news story as a string. All these news stories are then concatenated using the `join` command. We will only be using the first two million characters, just to keep things manageable in the space of a single lab session. Feel free to explore with using the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLOBjG8r_Xu9"
      },
      "source": [
        "with open('dataset.csv', newline='', encoding = \"ISO-8859-1\") as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    all_text = \"\".join(row['news'] for row in reader)\n",
        "\n",
        "all_text = all_text[:1000000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HUB6NPc_iz1"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHLcZcNzIe-T"
      },
      "source": [
        "As is commonly the case when dealing with natural language, we must first turn individual characters (or words if we have a word-level RNN) into indices (i.e. integer numbers). The best way to do this is using a python dictionary. In fact we will define python dictionaries that map an index to a character (`itoc`) as well as a character to its index (`ctoi`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljgPjk3Hjd-x"
      },
      "source": [
        "all_chars = sorted(set(all_text))\n",
        "num_of_characters = len(all_chars)\n",
        "itoc = {i:c for i,c in enumerate(all_chars)}\n",
        "ctoi = {c:i for i,c in enumerate(all_chars)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOpohOdhvZQ1"
      },
      "source": [
        "Now we can easily convert a sequence of characters into a sequence of numbers (indices). However as is usually the case with categorical variables (i.e. those that can take one out of a finite number of possible values) they must be converted into vectors (e.g. one-hot encodings) before we can process them further. The reasons are somewhat complex, but fundamentally it's because integer indices have a natural ordering. E.g. index 2 is somehow 'close' to index 3. If we would like our algorithm to take that into account then integer encodings are fine. Usually however we don't want to assume any ordering and in these cases we are obliged to use vector representations. \n",
        "\n",
        "In fact we will go one step further than simple one-hot encodings. We will define a general, learnable vector for each of the characters. This is easily achieved using what is known as an embedding layer.\n",
        "\n",
        "Consider just for illustration's sake a very simple 4-dimensional embedding that maps from a character index to a 4D vector. This is defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t15plAY3zy8n"
      },
      "source": [
        "embedding = nn.Embedding(num_of_characters, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcS19TP90NNi"
      },
      "source": [
        "This is a structure that produces a different 4-d vector for each of the different characters in our character set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55fwHg9-0a-5"
      },
      "source": [
        "embedding(torch.tensor(ctoi['s']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydsiy3O806W9"
      },
      "source": [
        "We are now ready to define a network that will predict the next character given some text. This is essentially a classification task. Because we want our network to be able to use input sequences of variable length we will use a recurrent network in the middle of the computation. Here we chose a GRU, but an LSTM will also work.\n",
        "\n",
        "The structure of the predictor network is as follows:\n",
        "\n",
        "input char sequence -> embedding -> GRU -> fully connected layer -> softmax\n",
        "\n",
        "As we will be using a `CrossEntropyLoss` loss function, we can omit the softmax layer in the end. The code looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlJg69E8_pPY"
      },
      "source": [
        "class NextCharPredictor(nn.Module):\n",
        "    def __init__(self, charset_size, embed_size=100, hidden_dim=512*2):\n",
        "        super(LSTMCharPred, self).__init__()\n",
        "        self.embedding = nn.Embedding(charset_size, embed_size)\n",
        "        self.charset_size = charset_size\n",
        "        self.lstm = nn.GRU(input_size=embed_size,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, charset_size)\n",
        "\n",
        "    def forward(self, x, batch_size=1):\n",
        "        x = self.embedding(x.view(batch_size,-1))\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        return x.view(-1,self.charset_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyb5tu0I4lu2"
      },
      "source": [
        "Note that RNNs in pytorch can handle mini-batches. If you use the `batch_first=True` flag, then the networ expects inputs of shape:\n",
        "\n",
        "batch_size x sequence_size x element_size\n",
        "\n",
        "\n",
        "where batch_size is the number of sequences contained in your batch, sequence_size is the length of each sequence and element_size is the size of the vector that represents each sequence element (in our case a character embedding vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo8dZbP2x0Pr"
      },
      "source": [
        "class RunningAverage:\n",
        "    def __init__(self):\n",
        "        self.n=0\n",
        "        self.tot=0\n",
        "    \n",
        "    def add(self,x):\n",
        "        self.n += 1\n",
        "        self.tot += x\n",
        "        \n",
        "    def __call__(self):\n",
        "        return self.tot/self.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTxGGn2J1YsE"
      },
      "source": [
        "tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnCQlM_a_sBU"
      },
      "source": [
        "num_of_epochs = 50\n",
        "seq_length = 100\n",
        "batch_size=64*2\n",
        "embed_size=100\n",
        "hidden_dim=512*2\n",
        "net = LSTMCharNextCharPredictorPred(embed_size=embed_size, hidden_dim=hidden_dim, charset_size = num_of_characters).to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(net.parameters(), lr=0.001) \n",
        "net.train()\n",
        "max_iter = int(len(all_text)/(batch_size*seq_length+1))\n",
        "\n",
        "writer = SummaryWriter(f'runs/exp_1M_rnd_batch128')\n",
        "\n",
        "for e in range(num_of_epochs):\n",
        "    train_acc = RunningAverage()\n",
        "    for i,txt in enumerate(sliced(all_text, batch_size*seq_length+1)):\n",
        "        if len(txt)<batch_size*seq_length+1:\n",
        "            break\n",
        "        x = torch.tensor([ctoi[c] for c in txt[:-1]], device = device)\n",
        "        t = torch.tensor([ctoi[c] for c in txt[1:]], device = device)\n",
        "        optim.zero_grad()\n",
        "        y = net(x,batch_size)\n",
        "        L = loss(y, t)\n",
        "        acc = sum(y.argmax(dim=1)==t).item()/(batch_size*seq_length)\n",
        "        train_acc.add(acc)\n",
        "        print(f\"\\rEpoch: {e}/{num_of_epochs} Iter: {i}/{max_iter}\\tacc={100*acc:0.2f}%\\tL={L}\", end=\"\")\n",
        "        L.backward()\n",
        "        optim.step()\n",
        "    writer.add_scalar('Accuracy', train_acc(), e)\n",
        "    writer.flush()\n",
        "    print(f\"\\rEpoch: {e}/{num_of_epochs} Average acc: {train_acc()}\")\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjZMBAn6ALCL"
      },
      "source": [
        "def generate_text(net, seed_txt, length=100):\n",
        "    seed_lst_idx = [ctoi[c] for c in seed_txt]\n",
        "    seed_idx = torch.tensor(seed_lst_idx, device = device)\n",
        "    # generated_text = generate_text(net, seed_idx, length=100)\n",
        "    net.train(False)\n",
        "    x=net.embedding(seed_idx.view(1,-1))\n",
        "    x,h = net.lstm(x)\n",
        "    x = net.fc(x)\n",
        "    out = x.argmax(dim=2).view(-1)\n",
        "    x = out[-1]\n",
        "    generated_text=[x.item()]\n",
        "    for i in range(length):\n",
        "        x=net.embedding(x.view(1,-1))\n",
        "        x,h = net.lstm(x,h)\n",
        "        x = net.fc(x)\n",
        "        x = x.argmax(dim=2).view(-1)\n",
        "        generated_text.append(x.item())\n",
        "    return  \"\".join(itoc[i] for i in seed_lst_idx+generated_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpcHXs1-NlnF"
      },
      "source": [
        "print(generate_text(net, \"The SEC's settlement with CAM and CFD included agreements with three other ex-managers\", length=200))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Q09K0_O19C"
      },
      "source": [
        "idx=all_text.index(\"On Monday\")\n",
        "all_text[idx-100:idx+100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo96Limu27jl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}