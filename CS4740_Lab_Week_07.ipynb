{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9o1hLrbVGAR4bBTRSGEm2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqFSK2Qrqam-"
      },
      "source": [
        "\n",
        "# [CS4740 Labs] Lab 7: Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CJ5q5hZqhEw"
      },
      "source": [
        "##Introduction\n",
        "\n",
        "In this lab we will look at one of the most exciting (and genuinely fun) aspects of Deep Learning which is the way deep neural networks can be taught to solve AI tasks. We will move from networks that make simple inferences (e.g. classification or regression) onto networks that can actually make decisions on which action to take, based on world observations received.\n",
        "\n",
        "The AI paradigm we will be using is Reinforcement Learning, a powerful abstraction of problem solving, which of course has been around for ages. The exciting part of deep learning, is that for the first time we were able to solve Reinforcement Learning problems from raw, unstructured observations, for example the screen pixels of Atari games, instead of hand-crafted features.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1vE2Z_KC__bhRqFeDiedOPfja7Rf7hZKF'/>\n",
        "<figcaption>Atari games</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "This type of Reinforcement Learning is of course a much more difficult problem but thankfuly deep learning technologies were designed for exactly these situations. It turns out that all the famous Q-learning algorithms can be given the deep learning, end-to-end training treatment, which in many cases leads to very elegant RL agents that can solve what appear to be very difficult tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNXC083lf5hW"
      },
      "source": [
        "Let's begin by installing a few libraries, that are required for rendering atari games on the jupyter notebook (colab) environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_1_Fotu_CrO"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1\n",
        "!apt install swig cmake > /dev/null 2>&1\n",
        "!pip install box2d box2d-kengz > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAV9ddq6gJqf"
      },
      "source": [
        "And let's also import the key modules for OpenAI gym and pytorch, and let's enable virtual displays for rendering the RL environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL6sR90gp7XB"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# To activate virtual display \n",
        "# need to run a script once for training an agent as follows\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVDtsbuMg2bI"
      },
      "source": [
        "## Tabular Q-learning with the Taxi driver task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnF_GDtUqZ3A"
      },
      "source": [
        "But before we get busy with the challenging Deep RL problems such as Atari, it's a good idea to start with something easier just so we refresh the fundamentals of RL.\n",
        "\n",
        "We will therefore start with the \"Taxi\" problem. This is a small-scale problem (with only 500 possible states) that models a taxi driver picking up passengers and dropping them off to various locations. Let's initialize the Taxi environment from OpenAI gym:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1nd9XPpXLn"
      },
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWm7kzI_pl8H"
      },
      "source": [
        "Let's now find out about the state/observation space and the action space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtVxRUVFkI3n"
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeIL1-2lp-OG"
      },
      "source": [
        "So this task has 500 different states and 6 possible actions. Let's see what the initial state looks like:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oBzF5IMqkE-"
      },
      "source": [
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ujZe68qkl3"
      },
      "source": [
        "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square (yellow box) and the passenger is at a random location (designated with a blue letter). The taxi drives to the passenger's location, picks up the passenger (becoming a green box when that happens), drives to the passenger's destination (another one of the four specified locations designated with a magenda letter), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "\n",
        "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
        "\n",
        "There are 6 discrete deterministic actions:\n",
        "0. move south\n",
        "1. move north\n",
        "2. move east\n",
        "3. move west\n",
        "4. pickup passenger\n",
        "5. drop off passenger\n",
        "\n",
        "\n",
        "The rewards given at each state for each action are as follows: There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD1bHN9isV5C"
      },
      "source": [
        "We can use `env.action_space.sample()` to sample a random action and the game is stepped through using the `env.step(action)` method that we saw in lectures. Let's try a completely random agent, using the `env.render(mode='ansi')` to render the game state into a string instead of `stdout`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Xv___pspTz"
      },
      "source": [
        "frames=[]\n",
        "\n",
        "done=False\n",
        "\n",
        "S = env.reset()\n",
        "while not done:\n",
        "    A = env.action_space.sample()\n",
        "    S_new, R, done, _ = env.step(A)\n",
        "    S=S_new\n",
        "    frames.append(env.render(mode='ansi'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44fWEAKbt-44"
      },
      "source": [
        "We can now print each state sequentially and see  the game running as an animation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0FKXQM1uWcL"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "for i,f in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(f\"{i+1}/{len(frames)}\")\n",
        "    print(f)\n",
        "    sleep(.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQJreg5SvPal"
      },
      "source": [
        "As you probably guessed already, this doesn't look great, because the agent is just randomly choosing actions to take. We need to apply some RL training to get some intelligent behaviour. One of the most well established algorithms, almost synonymous with RL over many decades, is Q-learning. The idea is very simple: We maintain a table of Q-values, i.e. estimates of how valuable a particular state/action combination is. And since we do not know the exact values for each state/action, we can use the previous Q-values stored, together with the reward currently obtained. \n",
        "\n",
        "The way this works is as follows:\n",
        "\n",
        "Suppose the agent finds itself in state $s$, and decides to perform action $a$ while in that state. Then the system gives out a reward $r$ and the new state $s'$. Our current estimate of the value of the current state action combination is $Q(s,a)$. Where can we get a better estimate? Well, we are currently receiving reward $r$ and we will be getting some future rewards, call them $G$, which we must discount to the present, using discount factor $\\gamma$. So the correct value we would like to store in $Q(s,a)$ is \n",
        "$$r + \\gamma G$$\n",
        "\n",
        "The only slight snag is that we don't know $G$, the value of all future rewards from state $s'$ onwards. But now we remember we have some $Q$ estimates already. Can we use those? Well, since our new state is $s'$, if we then take action $a'$, the total amount of reward we expect to get is currently estimated as $Q(s',a')$. But of course we will not be choosing any old action $a'$! We will be choosing the best one, i.e. the one that maximizes $Q(s',a')$. So the value we can expect to get from state $s'$ is currently estimated as:\n",
        "$$\\max_{a'} Q(s',a').$$\n",
        "Putting that together with the current reward $r$, our estimate for the value of the $s$,$a$ combination becomes:\n",
        "$$r+\\gamma \\max_{a'} Q(s',a')$$\n",
        "\n",
        "This expression above we will call the 'target'. The basis of the Q-learning algorithm is the computation of these target values, and the gradual pushing of the old Q-values towards the targets. In plain old vanilla Q-learning, in each iteration we will be moving the old Q-value a fraction of the way towards the target. The fraction is given by parameter $\\alpha$ so the final update formula looks like:\n",
        "\n",
        "$$Q(s,a) := (1-\\alpha)Q(s,a) + \\alpha (r+\\gamma \\max_{a'} Q(s',a'))$$\n",
        "\n",
        "Because this version of the problem essentially involves creating a table of Q-values, it is known as tabular Q-learning (or just Q-learning). There are many options in python for the data structure that will represent our Q-value estimates. The most elegant is through a dictionary `Q` who's keys are the states. For each state `S`, it records an\n",
        "numpy array `[q1,q2,...,q6]` that stores our current estimates for the values of the six possible actions we can take at state `S`. So to find the value of the combination of state `S` with action `A` (where `A` is a number betwee 0 and 5) we would write\n",
        "\n",
        "`Q[S][A]`\n",
        "\n",
        "To get the value of the best action when in state `S` we would use the `.max()` method on the numpy array, to give\n",
        "\n",
        "`Q[S].max()`\n",
        "\n",
        "And to find out the index of that best action (useful when we are selecting which action to follow) we would use the `.argmax()` method. So the best action to follow when in state `S` would be given by:\n",
        "\n",
        "`Q[S].argmax()`\n",
        "\n",
        "Let's see the python implementation of tabular Q-learning in full:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXJAg3mJ1Gsp"
      },
      "source": [
        "from collections import defaultdict\n",
        "from random import random,choice\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# Q-learning parameters\n",
        "alpha = 0.5\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "num_of_episodes = 3000\n",
        "\n",
        "# Q[S][A] stores the estimated value of taking action A when in state S\n",
        "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "frames,rewards=[],[]\n",
        "\n",
        "for e in range(num_of_episodes):\n",
        "    S = env.reset()\n",
        "    done=False\n",
        "    totalR=0\n",
        "    while not done:\n",
        "        # Epsilon-greedy strategy        \n",
        "        A = env.action_space.sample() if random()<epsilon else Q[S].argmax()\n",
        "\n",
        "        # Executing action, receiving reward and new state\n",
        "        S_new, R, done, _ = env.step(A)\n",
        "                \n",
        "        # target is an estimate of the value of current state and action\n",
        "        # It is given by Bellman update formula if the state is non-terminal\n",
        "        # For final states, it is just the reward received.\n",
        "        target = R + gamma * Q[S_new].max() if not done else R\n",
        "\n",
        "        # moving the existing Q-value towards the target by a factor of alpha\n",
        "        Q[S][A] += alpha * (target - Q[S][A])\n",
        "\n",
        "        # update the state\n",
        "        S = S_new\n",
        "        \n",
        "        # monitoring of training progress\n",
        "        totalR += R\n",
        "        # picking a few episodes to record as animation sequences.\n",
        "        if e in [100,200,995,996,997,998,999]: \n",
        "            frames.append({'e':e, 'frame':env.render(mode='ansi')})\n",
        "    rewards.append(totalR)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lizsMpKYlIQQ"
      },
      "source": [
        "We can now plot the rewards received at each training episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdsYE-Q6lNUq"
      },
      "source": [
        "plt.plot(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1JLeX2wlN0J"
      },
      "source": [
        "The plot seems to indicate the algorithm has succeeded but it is a bit noisy, due to the random element in the task. We can remove some of that noise by computing a rolling average of the rewards. The numpy `convolve` method is quite handy for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mDFNbEAlPQx"
      },
      "source": [
        "plt.plot(np.convolve(rewards, [0.01]*100,'valid'),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvDHpiNNl02r"
      },
      "source": [
        "The algorithm seems to be earning a good amount of rewards. But what about the policy it has learned? Can it actually solve the task? Let's look at the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMtAATGCRAQ8"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "for f in frames:\n",
        "    clear_output(wait=True)\n",
        "    print(f\"e={f['e']+1}\")\n",
        "    print(f['frame'])\n",
        "    sleep(.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxPwaqixmfVA"
      },
      "source": [
        "After some uncertain early runs it seems that towards the end Q-learning has indeed learned the correct policy. The taxi agent is able to ferry passengers to and from each of the four locations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACo7-iBB4ofU"
      },
      "source": [
        "## Lunar Landing with DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w1fet-K41Rq"
      },
      "source": [
        "We are now ready to tackle something a bit more challenging. The Lunar Lander task is a fairly simplified simulator of a lunar lander module attempting to land on the moon with the help of three thruster engines. An episode always begins with the lander module descending from the top of the screen. In each time step the simulator provides us with the horizontal and vertical positions, orientation, linear and angular velocities and the state of each landing leg (left and right). This is an 8-dimensional vector.\n",
        "\n",
        "There are 4 possible actions: do nothing, fire left  engine, fire main engine, or fire right  engine. By executing the right combination of these actions in the right time, our lander must make it safely on to the landing pad. The reward is suitably designed to encourage the correct type of landing. The task description states:\n",
        "\n",
        "\"Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points. If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points. Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\"\n",
        "\n",
        "Let's create the environment and check out how the random agent performs. In order to visualise the lunar module landing, we will wrap the environment within a `Monitor` wrapper. These are special classes that offer some additional features to the standard environments. The `Mirror` wrapper saves videos of runs of the environment. The following line will ensure that videos of environment runs will be placed on the `./video` subdirectory which you can find by clicking on the file browser tab (bottom-most  tab on the left in the colab environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLOI8DOAkTWj"
      },
      "source": [
        "env = Monitor(gym.make('LunarLander-v2'),directory='./video',force=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqmkVxqXwog2"
      },
      "source": [
        "Let's run a single episode with the random agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgEPb9TL_wVB"
      },
      "source": [
        "s = env.reset()\n",
        "done=False\n",
        "while not done:\n",
        "    a = env.action_space.sample()\n",
        "    s,r,done,_=env.step(a)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lMDIQMNwsEb"
      },
      "source": [
        "Now we can download the video from the file directory tab and play it. The lander just crashes onto the ground helplessly.\n",
        "\n",
        "This problem has a continuous 8-dimensional state space, which means that we cannot tabulate the Q-values. One possible solution is to quantize this space and then treat it as discrete. This is not always easy or possible since, even an 8-dimensional state can lead to some $100^8=1e16$ states if we quantize each dimension into 100 intervals. \n",
        "\n",
        "We will instead opt for a different approach. We will use a neural network to store the Q-values. The reason this works is because Q-values for nearby states can be expected to be similar. Which means we don't need separate rows in a Q-value table. The neural network can describe the mapping from state vector to Q-values. Let's define our old friend, the `MLPnet` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NKKT4fUHrUJ"
      },
      "source": [
        "class MLPnet(nn.Module):\n",
        "    def __init__(self, *sizes):\n",
        "        self.num_actions = sizes[-1]\n",
        "        super(MLPnet, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for s,s_ in zip(sizes[:-1],sizes[1:]):\n",
        "            self.layers.append(nn.Linear(s,s_))\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXs--5eY1jwY"
      },
      "source": [
        "We also need to define our own convenience wrapper that maps state vectors and rewards to `torch.tensors`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYlJi4q9MVro"
      },
      "source": [
        "class TorchTensor(gym.ObservationWrapper, gym.RewardWrapper):\n",
        "    def observation(self, observation):\n",
        "        return torch.tensor(observation, device=device, dtype=torch.float)\n",
        "    def reward(self, reward):\n",
        "        return torch.tensor(reward, device=device, dtype=torch.float)\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        return self.observation(observation), self.reward(reward), done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcuN3Yrc2JUz"
      },
      "source": [
        "Let us now create the lander environment wrapped in a `Monitor` and a `TorchTensor` wrapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGGyU1RH23sb"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = TorchTensor(Monitor(gym.make('LunarLander-v2'),directory='./video',force=True,video_callable=lambda i:i%10==0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je6rGELZ26Ys"
      },
      "source": [
        "We will be creating a feedforward network with input size of 8 and output size of 4 and a couple of hidden layers of sizes 50 and 100. It is however good practice to read state and action space dimensions from the `env` object as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePA-snhJ3Bsm"
      },
      "source": [
        "num_actions = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "qnet = MLPnet(state_size,50,100,num_actions).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COpf1asw3nqL"
      },
      "source": [
        "This ensures that our code can be applied to many different environments with minimum changes. Let's apply the network on a state vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADHaTU-M31Uk"
      },
      "source": [
        "S = env.reset()\n",
        "\n",
        "print(qnet(S))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy_DdiDu37s0"
      },
      "source": [
        "The output of the Q network can be interpreted as before: a 4D vector that stores the q-values of each of the 4 actions, taken while at state S. The Q-learning algorithm will stay largely the same, with the only difference being that instead of the Q-values being updated with a linear update formulat, we update the q-network weights, using a back-propagation step.\n",
        "\n",
        "To keep the code tidy we can define the update of the network weights as a function that computes a loss function between the existing network function and a target. It then executes a `.backward()` step on that loss function and then a backprop step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bO-eggf5fG_"
      },
      "source": [
        "def updateQNet(qvalue, target, optimiser, loss):\n",
        "    L = loss(qvalue, target)\n",
        "    optimiser.zero_grad()\n",
        "    L.backward()\n",
        "    optimiser.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQeKTVsTz8JY"
      },
      "source": [
        "We can now write the full Q-learning code using the Q-network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI0bZR3K5-SW"
      },
      "source": [
        "from random import random\n",
        "\n",
        "epsilon = 0.1\n",
        "rewards=[]\n",
        "gamma = 0.9\n",
        "num_episodes=200\n",
        "num_actions = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "\n",
        "\n",
        "qnet = MLPnet(state_size,50,100,num_actions).to(device)\n",
        "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "for e in range(num_episodes):\n",
        "    done = False\n",
        "    S = env.reset()\n",
        "    tot_reward=0\n",
        "\n",
        "    # epsilon = max(0.1, epsilon*0.99)\n",
        "    while not done:\n",
        "\n",
        "        # Epsilon-greedy strategy\n",
        "        A = env.action_space.sample() if random()<epsilon else qnet(S).argmax().item()\n",
        "\n",
        "        # Executing action, receiving reward and new state\n",
        "        S_new, R, done, _ = env.step(A)\n",
        "\n",
        "        # target is an estimate of the value of current state and action\n",
        "        # It is given by Bellman update formula if the state is non-terminal\n",
        "        # For final states, it is just the reward received.\n",
        "        target = R + gamma * qnet(S_new).max() if not done else R\n",
        "\n",
        "        # moving the existing Q-value towards the target by backpropagating on \n",
        "        # an MSE loss between the old value and the target.\n",
        "        updateQNet(qnet(S)[A], target, optim, loss)\n",
        "\n",
        "        # update the state        \n",
        "        S = S_new\n",
        "\n",
        "        tot_reward += R.item()\n",
        "        \n",
        "    rewards.append(tot_reward)\n",
        "    print(f\"\\r{e}/{num_episodes} tot_reward={tot_reward}\",end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqeTrn9g0Gqr"
      },
      "source": [
        "Let's plot the training performance. We can also take a look at the videos taken during training, which we can find in the `video` subfolder on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mZpZNsHx1M"
      },
      "source": [
        "plt.plot(np.convolve(rewards,[0.01]*100,'valid'),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt4304Wz0Sci"
      },
      "source": [
        "It seems to be learning but very very slowly. The reason is that we are essentially using a mini-batch of \"one\" at each iteration. Our q-network update only uses the latest experience, the transition from state `S` to `S_new` with reward `R`, after taking action `A`. \n",
        "\n",
        "But a moment's thought should convince you that the latest experience is exactly that: just a single experience, of equal weight as any other experience when it comes to Q-learning. So a better formulation could re-use a larger set of past experiences at each time-step.  So with every single backprop step our q-network becomes a better approximation to the true q-value function, giving us the well-known efficiencies of mini-batch backpropagation. We will see how this works in practice next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvf-t-ms6NLC"
      },
      "source": [
        "## DQN with a replay buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01XWW4F7uqf"
      },
      "source": [
        "Our strategy for creating a mini-batch version of q-learning, is to store each new experience that comes in, in a buffer. Each experience needs to record a `(S,A,R,S_new,done)` tuple. I.e. the current state, the action taken, the reward obtained and the new state this led to as well as a boolean that signifies whether the terminal state has been reached.\n",
        "\n",
        "To keep things manageable computationally we only need to store the last few experiences so we can do this either with a cyclical buffer, or with a simple deque object. This is a special type of list that has an maximum length. If we try to append an element when the max length has been reached, the deque will pop the oldest element it holds.\n",
        "\n",
        "Just a warning, however, if you want a really efficient implementation, it's probably better and faster to write your own circular buffer data structure. Here we'll just use a deque for convenience.\n",
        "\n",
        "To define a deque with a max length of 10000, we just write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rSHd0Tn9ITh"
      },
      "source": [
        "from collections import deque\n",
        "replaybuffer = deque(maxlen=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6WaEwSD9S3e"
      },
      "source": [
        "We can then store new experiences using\n",
        "\n",
        "`replaybuffer.append((S,A,R,S_new,done))`\n",
        "\n",
        "To see the details of how this will work, let's populate a replaybuffer with some experiences from a random agent during 5 episodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Fw1g40948e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = TorchTensor(Monitor(gym.make('LunarLander-v2'),directory='./video',force=True,video_callable=lambda i:i%10==0))\n",
        "\n",
        "replaybuffer = deque(maxlen=1000)\n",
        "\n",
        "for e in range(5):\n",
        "    done = False\n",
        "    S = env.reset()\n",
        "    while not done:\n",
        "        A = env.action_space.sample()\n",
        "        S_new, R, done, _ = env.step(A)\n",
        "        replaybuffer.append((S,A,R,S_new,done))\n",
        "        S = S_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-kOvvU1-hqi"
      },
      "source": [
        "Now our `replaybuffer` deque will contain a number of `(S,A,R,S_new,done)` tuples. How do we convert these into minibatches?\n",
        "\n",
        "There is a very elegant python formulation we can use to convert a list of tuples to a tuple of lists. \n",
        "\n",
        "If `batch` contains a list of `(S,A,R,S_new,done)` tuples, then `zip(*batch)` will be a tuple that contains: a list of `S` tensors, a list of `A`'s, a list of `R`'s etc. \n",
        "\n",
        "If we run\n",
        "\n",
        "`s,a,r,s_new,d = zip(*batch)` \n",
        "\n",
        "we can easily extract the lists that we seek. We would then need to convert them into tensors in order to create the target tensor. We can use the super-useful `tensor.where` function that can implement the \n",
        "\n",
        "`R+gamma*qmax if done else R` \n",
        "\n",
        "construct in a single line. Finally we can use the `.gather()` method of tensors, in order to pick out the q value corresponding to the action taken. Finally we calculate the loss function between the q-values and the targets and we are ready to backpropagate. All this can be encapsulated in a single update function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsOnBdQP22SP"
      },
      "source": [
        "def updateQNet_batch(qnet, batch, optimizer, loss):\n",
        "    s,a,r,s_new,d = batch\n",
        "    \n",
        "    s=torch.stack(s)\n",
        "    s_new=torch.stack(s_new)\n",
        "    r=torch.tensor(r,device=device,dtype=torch.float)\n",
        "    d=torch.tensor(d,device=device,dtype=torch.bool)\n",
        "    a=torch.tensor(a,device=device,dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        qmax,_ = qnet(s_new).view(-1,num_actions).max(dim=1)\n",
        "        target = torch.where(d, r, r + gamma * qmax).view(-1,1)\n",
        "    L = loss(qnet(s).gather(1,a.view(-1,1)),target)\n",
        "    optim.zero_grad()\n",
        "    L.backward()\n",
        "    optim.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRm5MIMEAL8"
      },
      "source": [
        "Let's put everything together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkBI-wkqKiBC"
      },
      "source": [
        "from collections import deque\n",
        "from random import random,sample\n",
        "\n",
        "env = TorchTensor(Monitor(gym.make('LunarLander-v2'),directory='./video',force=True,video_callable=lambda i:i%10==0))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "epsilon = 0.1\n",
        "rewards=[]\n",
        "gamma = 0.99\n",
        "num_episodes=1000\n",
        "num_actions = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "batchsize=64\n",
        "\n",
        "replaybuffer = deque(maxlen=100000)\n",
        "\n",
        "qnet = MLPnet(state_size,50,100,num_actions).to(device)\n",
        "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "for e in range(num_episodes):\n",
        "    done = False\n",
        "    S = env.reset()\n",
        "    tot_reward=0\n",
        "\n",
        "    # epsilon = max(0.1, epsilon*0.99)\n",
        "    while not done:\n",
        "        # Epsilon-greedy strategy\n",
        "        A = env.action_space.sample() if random()<epsilon else qnet(S).argmax().item()\n",
        "\n",
        "        # Executing action, receiving reward and new state\n",
        "        S_new, R, done, _ = env.step(A)\n",
        "\n",
        "        # adding the latest experience onto the replay buffer\n",
        "        replaybuffer.append((S,A,R,S_new,done))\n",
        "\n",
        "        S = S_new\n",
        "        if len(replaybuffer)>=batchsize:\n",
        "            batch = sample(replaybuffer, batchsize)\n",
        "            batch = zip(*batch)    \n",
        "            updateQNet_batch(qnet, batch, optim, loss)\n",
        "\n",
        "        tot_reward += R\n",
        "\n",
        "    rewards.append(tot_reward)\n",
        "    print(f\"\\r{e}/{num_episodes} tot_reward={tot_reward}\",end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-kpRsZJEt2h"
      },
      "source": [
        "We can now check out the progress of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HyR_Gj0JPLW"
      },
      "source": [
        "plt.plot(np.convolve(rewards,[0.01]*100,'valid'),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcmXBxx7FBjh"
      },
      "source": [
        "Both the training curve and the videos show that this time the agent has done a much better job. It almost always manages to land without crashing and a lot of the time between the flags too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lOKUB6-A-9"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You have all the ingredients you need to try your hand on one of the atari games! Choose one of the easiest examples e.g. pong or breakout.\n",
        "\n",
        "Write the DQN algorithm for an atari game, using an appropriately initialised network. You have to modify the `TensorWrapper` code so that it produces tensor observations from the pixels of the environment, which you can get using \n",
        "\n",
        "`env.render(mode='rgb_array')`\n",
        "\n",
        "What's more, you will have to maintain inside the `TensorWrapper` object, a small deque buffer of the last N pixel frames, so that you can give your network a sense of motion during the game. You can then either give as observation the N x Width x Height tensor to be processed by a convnet, or you can flatten it into a linear vector to be processed by an appropriate `MLPnet`. \n",
        "\n",
        "Another option for simplifying computation is to subsample the pixel frames into a more manageable lower resolution. \n",
        "\n",
        "Good luck!\n",
        "\n"
      ]
    }
  ]
}