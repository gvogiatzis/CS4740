{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4740_Lab_Week_07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEjE8wzbXja4mmfPjA77Tz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS4740/blob/main/CS4740_Lab_Week_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqFSK2Qrqam-"
      },
      "source": [
        "\n",
        "# [CS4740 Labs] Lab 7: Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CJ5q5hZqhEw"
      },
      "source": [
        "##Introduction\n",
        "\n",
        "In this lab we will look at one of the most exciting (and genuinely fun) aspects of Deep Learning which is the way deep neural networks can be taught to solve AI tasks. We will move from networks that make simple inferences (e.g. classification or regression) onto networks that can actually make decisions on which action to take, based on world observations received.\n",
        "\n",
        "The AI paradigm we will be using is Reinforcement Learning, a powerful abstraction of problem solving, which of course has been around for ages. The exciting part of deep learning, is that for the first time we were able to solve Reinforcement Learning problems from raw, unstructured observations, for example the screen pixels of Atari games, instead of hand-crafted features.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1vE2Z_KC__bhRqFeDiedOPfja7Rf7hZKF'/>\n",
        "<figcaption>Atari games</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "This type of Reinforcement Learning is of course a much more difficult problem but thankfuly deep learning technologies were designed for exactly these situations. It turns out that all the famous Q-learning algorithms can be given the deep learning, end-to-end training treatment, which in many cases leads to very elegant RL agents that can solve what appear to be very difficult tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNXC083lf5hW"
      },
      "source": [
        "Let's begin by installing a few libraries, that are required for rendering atari games on the jupyter notebook (colab) environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_1_Fotu_CrO"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1\n",
        "!apt install swig cmake > /dev/null 2>&1\n",
        "!pip install box2d box2d-kengz > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAV9ddq6gJqf"
      },
      "source": [
        "And let's also import the key modules for OpenAI gym and pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL6sR90gp7XB"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTzD9RbJ5LsS",
        "outputId": "be71b979-f8ab-4eea-aee8-05c4fa9a0718"
      },
      "source": [
        "# To activate virtual display \n",
        "# need to run a script once for training an agent as follows\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fea536562e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnF_GDtUqZ3A"
      },
      "source": [
        "But before we get busy with the challenging Deep RL problems such as Atari, it's a good idea to start with something easier just so we refresh the fundamentals of RL.\n",
        "\n",
        "We will therefore start with the \"Taxi\" problem. This is a small-scale problem (with only 500 possible states) that models a taxi driver picking up passengers and dropping them off to various locations. Let's initialize the Taxi environment from OpenAI gym:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1nd9XPpXLn"
      },
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWm7kzI_pl8H"
      },
      "source": [
        "Let's now find out about the state/observation space and the action space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtVxRUVFkI3n",
        "outputId": "f275157f-5614-4533-f7d6-88b9a2d8d87b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(500)\n",
            "Discrete(6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeIL1-2lp-OG"
      },
      "source": [
        "So this task has 500 different states and 6 possible actions. Let's see what the initial state looks like:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oBzF5IMqkE-",
        "outputId": "13d5cac9-ebd3-4606-aa40-828f158fc677",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ujZe68qkl3"
      },
      "source": [
        "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square (yellow box) and the passenger is at a random location (designated with a blue letter). The taxi drives to the passenger's location, picks up the passenger (becoming a green box when that happens), drives to the passenger's destination (another one of the four specified locations designated with a magenda letter), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "\n",
        "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
        "\n",
        "There are 6 discrete deterministic actions:\n",
        "0. move south\n",
        "1. move north\n",
        "2. move east\n",
        "3. move west\n",
        "4. pickup passenger\n",
        "5. drop off passenger\n",
        "\n",
        "\n",
        "The rewards given at each state for each action are as follows: There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD1bHN9isV5C"
      },
      "source": [
        "We can use `env.action_space.sample()` to sample a random action and the game is stepped through using the `env.step(action)` method that we saw in lectures. Let's try a completely random agent, using the `env.render(mode='ansi')` to render the game state into a string instead of `stdout`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Xv___pspTz"
      },
      "source": [
        "frames=[]\n",
        "\n",
        "done=False\n",
        "\n",
        "S = env.reset()\n",
        "while not done:\n",
        "    A = env.action_space.sample()\n",
        "    S_new, R, done, _ = env.step(A)\n",
        "    S=S_new\n",
        "    frames.append(env.render(mode='ansi'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44fWEAKbt-44"
      },
      "source": [
        "We can now print each state sequentially and see  the game running as an animation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0FKXQM1uWcL",
        "outputId": "292483d3-73bd-47e1-8687-9f9d147caef9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "for i,f in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(f\"{i+1}/{len(frames)}\")\n",
        "    print(f)\n",
        "    sleep(.2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQJreg5SvPal"
      },
      "source": [
        "As you probably guessed already, this doesn't look great. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCtdrJR02ZqR"
      },
      "source": [
        "from random import random,choice"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "eXJAg3mJ1Gsp",
        "outputId": "4c033ee8-d64b-48bb-b42c-0ca2086a0b85"
      },
      "source": [
        "from collections import defaultdict\n",
        "from random import random\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "alpha = 0.5\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "num_of_episodes = 3000\n",
        "\n",
        "# Q is a dictionary that holds the \n",
        "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "frames,rewards=[],[]\n",
        "\n",
        "for e in range(num_of_episodes):\n",
        "    S = env.reset()\n",
        "    done=False\n",
        "    totalR=0\n",
        "    while not done:\n",
        "        # Epsilon greedy strategy        \n",
        "        A = env.action_space.sample() if random()<epsilon else Q[S].argmax()\n",
        "\n",
        "        # Executing action, receiving reward and new state\n",
        "        S_new, R, done, _ = env.step(A)\n",
        "                \n",
        "        # target is an estimate of the value of current state and action\n",
        "        # It is given by Bellman update formula if the state is non-terminal\n",
        "        # For final states, it is just the reward received.\n",
        "        target = R + gamma * Q[S_new].max() if not done else R\n",
        "\n",
        "        # moving the existing Q-value towards the target by a factor of alpha\n",
        "        Q[S][A] += alpha * (target - Q[S][A])\n",
        "        S = S_new\n",
        "        \n",
        "        # monitoring of training progress\n",
        "        totalR += R\n",
        "        if e in [100,200,995,996,997,998,999]:\n",
        "            frames.append({'e':e, 'frame':env.render(mode='ansi')})\n",
        "    rewards.append(totalR)\n",
        "env.close()\n",
        "\n",
        "# plt.plot(rewards,'-')\n",
        "plt.plot(np.convolve(rewards, [0.01]*100,'valid'),'-')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe9ef823d30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMtAATGCRAQ8"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "for f in frames:\n",
        "    clear_output(wait=True)\n",
        "    print(f\"e={f['e']+1}\")\n",
        "    print(f['frame'])\n",
        "    sleep(.2)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLOI8DOAkTWj"
      },
      "source": [
        "env = Monitor(gym.make('LunarLander-v2'),directory='./video',force=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OU1J23pYJPp"
      },
      "source": [
        "env.action_space."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuvTUHFCFuvw"
      },
      "source": [
        "At each step, the agent is provided with the current state of the space vehicle which is an 8-dimensional vector of real values, indicating the horizontal and vertical positions, orientation, linear and angular velocities, state of each landing leg (left and right). The agent then has to make one of four possible actions, namely do nothing, fire left orientation engine, fire main engine, or fire right orientation engine. These are 4 levers the agent must learn to control in order to land safely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgEPb9TL_wVB"
      },
      "source": [
        "s = env.reset()\n",
        "done=False\n",
        "while not done:\n",
        "    a = env.action_space.sample()\n",
        "    s,r,done,_=env.step(a)\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NKKT4fUHrUJ"
      },
      "source": [
        "class MLPnet(nn.Module):\n",
        "    def __init__(self, *sizes):\n",
        "        self.num_actions = sizes[-1]\n",
        "        super(MLPnet, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for s,s_ in zip(sizes[:-1],sizes[1:]):\n",
        "            self.layers.append(nn.Linear(s,s_))\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYlJi4q9MVro"
      },
      "source": [
        "class TorchWrapper(gym.ObservationWrapper):\n",
        "    def observation(self, observation):\n",
        "        return torch.tensor(observation, device=device, dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlsP11ElJDqY"
      },
      "source": [
        "def epsilon_greedy(epsilon, S, qnet, env):\n",
        "    if random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return qnet(S).argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-t15TVL2xlK"
      },
      "source": [
        "def sample(data, batchsize):\n",
        "    import random\n",
        "    size = min(len(data),batchsize)\n",
        "    return random.sample(data, size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bO-eggf5fG_"
      },
      "source": [
        "def updateQNet(qvalue, target, optimiser, loss):\n",
        "    L = loss(qvalue, target)\n",
        "    optimiser.zero_grad()\n",
        "    L.backward()\n",
        "    optimiser.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxxOCU9bHP8J"
      },
      "source": [
        "from collections import deque\n",
        "from random import random,sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI0bZR3K5-SW"
      },
      "source": [
        "env = TorchWrapper(Monitor(gym.make('LunarLander-v2'),directory='./video',force=True,video_callable=lambda i:i%10==0))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "epsilon = 0.4\n",
        "rewards=[]\n",
        "gamma = 0.99\n",
        "num_episodes=1000\n",
        "num_actions = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "batchsize=64\n",
        "\n",
        "\n",
        "qnet = MLPnet(state_size,64,64,32,num_actions)\n",
        "optim = torch.optim.Adam(qnet.parameters(), lr= 0.1)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "for e in range(num_episodes):\n",
        "    done = False\n",
        "    S = env.reset()\n",
        "    tot_reward=0\n",
        "\n",
        "    epsilon = max(0.1, epsilon*0.99)\n",
        "    while not done:\n",
        "        A = epsilon_greedy(epsilon, S, qnet, env) \n",
        "        S_new, R, done, _ = env.step(A)\n",
        "\n",
        "        R = torch.tensor(R,device=device, dtype=torch.float)\n",
        "\n",
        "        if done:\n",
        "            target = R\n",
        "        else:\n",
        "            target = R + gamma * qnet(S_new).max()\n",
        "        qvalue = qnet(S)[A]\n",
        "        updateQNet(qvalue, target,optim,loss)\n",
        "\n",
        "        tot_reward += R\n",
        "        S = S_new\n",
        "        \n",
        "    rewards.append(tot_reward)\n",
        "    print(f\"\\r{e}/{num_episodes} tot_reward={tot_reward}\",end='')\n",
        "\n",
        "\n",
        "# print(f\"Average reward:{sum(rewards[-50:])/50}\")\n",
        "# return rewards, replaybuffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsOnBdQP22SP"
      },
      "source": [
        "def updateQNet_batch(qnet, batch, optimizer, loss):\n",
        "    s,a,r,s_new,d = batch\n",
        "    \n",
        "    s=torch.stack(s)\n",
        "    s_new=torch.stack(s_new)\n",
        "    r=torch.tensor(r,device=device,dtype=torch.float)\n",
        "    d=torch.tensor(d,device=device,dtype=torch.bool)\n",
        "    a=torch.tensor(a,device=device,dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        qmax,_ = qnet(s_new).view(-1,num_actions).max(dim=1)\n",
        "        target = torch.where(d, r, r + gamma * qmax).view(-1,1)\n",
        "    L = loss(qnet(s).gather(1,a.view(-1,1)),target)\n",
        "    optim.zero_grad()\n",
        "    L.backward()\n",
        "    optim.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkBI-wkqKiBC"
      },
      "source": [
        "env = TorchWrapper(Monitor(gym.make('LunarLander-v2'),directory='./video',force=True,video_callable=lambda i:i%10==0))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "epsilon = 1\n",
        "rewards=[]\n",
        "gamma = 0.99\n",
        "num_episodes=1000\n",
        "num_actions = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "batchsize=64\n",
        "\n",
        "replaybuffer = deque(maxlen=100000)\n",
        "\n",
        "\n",
        "qnet = MLPnet(state_size,64,64,32,num_actions)\n",
        "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "for e in range(num_episodes):\n",
        "    done = False\n",
        "    S = env.reset()\n",
        "    tot_reward=0\n",
        "\n",
        "    epsilon = max(0.1, epsilon*0.99)\n",
        "    while not done:\n",
        "        A = epsilon_greedy(epsilon, S, qnet, env) \n",
        "        S_new, R, done, _ = env.step(A)\n",
        "        replaybuffer.append((S,A,R,S_new,done))\n",
        "\n",
        "        tot_reward += R\n",
        "        S = S_new\n",
        "        batch = sample(replaybuffer,batchsize)\n",
        "        batch = zip(*batch)\n",
        "        \n",
        "        updateQNet_batch(qnet, batch, optimizer, loss)\n",
        "    rewards.append(tot_reward)\n",
        "    print(f\"\\r{e}/{num_episodes} tot_reward={tot_reward}\",end='')\n",
        "\n",
        "\n",
        "# print(f\"Average reward:{sum(rewards[-50:])/50}\")\n",
        "# return rewards, replaybuffer"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}